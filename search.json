[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "If you’re here, you’re likely about to start work with the UCL Health Algorithms Laboratory.",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "people/academic/frost_thomas.html",
    "href": "people/academic/frost_thomas.html",
    "title": "Thomas Frost",
    "section": "",
    "text": "Dr. Thomas Frost is a clinical doctor currently studying for his PhD with the UKRI UCL Centre for Doctoral Training in AI-enabled Healthcare Systems.\nHis interests in AI and machine learning were first piqued by the wave of deep reinforcement learning work that came out in the mid-2010s. It wasn’t long before he found himself searching for opportunities to apply AI to healthcare, and the CDT provided the perfect opportunity to learn the skills to achieve this.\nHis current work is looking at applying RL algorithms to optimise IV infusions in Critical Care at UCLH. Prior to this, he completed his Master of Research thesis in 2022, which focused on identifying temporal ventilatory phenotypes in patients with COVID-19-related ARDS using Hidden Markov Models."
  },
  {
    "objectID": "people/academic/frost_thomas.html#biography",
    "href": "people/academic/frost_thomas.html#biography",
    "title": "Thomas Frost",
    "section": "",
    "text": "Dr. Thomas Frost is a clinical doctor currently studying for his PhD with the UKRI UCL Centre for Doctoral Training in AI-enabled Healthcare Systems.\nHis interests in AI and machine learning were first piqued by the wave of deep reinforcement learning work that came out in the mid-2010s. It wasn’t long before he found himself searching for opportunities to apply AI to healthcare, and the CDT provided the perfect opportunity to learn the skills to achieve this.\nHis current work is looking at applying RL algorithms to optimise IV infusions in Critical Care at UCLH. Prior to this, he completed his Master of Research thesis in 2022, which focused on identifying temporal ventilatory phenotypes in patients with COVID-19-related ARDS using Hidden Markov Models."
  },
  {
    "objectID": "people/academic/harris_steve.html",
    "href": "people/academic/harris_steve.html",
    "title": "Steve Harris",
    "section": "",
    "text": "Steve Harris is an Critical Care Consultant at UCLH, and Principal Research Fellow in Translational Data Science at UCL Institute of Health Informatics. He has held fellowships from Wellcome, and the Health Foundation, and won more than £5m in grant funding (UCLH Charity, EPSRC, NIHR/NHS-X) in the last 5 years. He is a co-investigator in CHIMERA, the Wellcome Innovation Flagship Critical Care Asia, and co-leads the NIHR Health Informatics Collaborative for Critical Care. At UCLH, he led the implementation of the Experimental Medicine Application Platform (EMAP) that is bringing machine learning back to the bedside leading to commendations from HDR-UK for its role in the pandemic response. He co-directs the data science training programme for UCLH."
  },
  {
    "objectID": "people/academic/harris_steve.html#biography",
    "href": "people/academic/harris_steve.html#biography",
    "title": "Steve Harris",
    "section": "",
    "text": "Steve Harris is an Critical Care Consultant at UCLH, and Principal Research Fellow in Translational Data Science at UCL Institute of Health Informatics. He has held fellowships from Wellcome, and the Health Foundation, and won more than £5m in grant funding (UCLH Charity, EPSRC, NIHR/NHS-X) in the last 5 years. He is a co-investigator in CHIMERA, the Wellcome Innovation Flagship Critical Care Asia, and co-leads the NIHR Health Informatics Collaborative for Critical Care. At UCLH, he led the implementation of the Experimental Medicine Application Platform (EMAP) that is bringing machine learning back to the bedside leading to commendations from HDR-UK for its role in the pandemic response. He co-directs the data science training programme for UCLH."
  },
  {
    "objectID": "people/academic/catling_finn.html",
    "href": "people/academic/catling_finn.html",
    "title": "Finn Catling",
    "section": "",
    "text": "I am an academic Anaesthetic and Critical Care doctor, and a Wellcome Trust 4i Clinical PhD Fellow at Imperial College London.\nMy research focuses on early diagnosis and improved treatment of critical illness, using methods from Bayesian statistics and machine learning. I am particularly interested in merging these methods with physiological models to provide bedside decision support, in disease heterogeneity, and in the role of uncertainty in clinical decision-making.\nMy recent work includes:\n\nUncertainty-aware mortality risk prediction for patients undergoing emergency laparotomy. Our prediction model is available as a web app and API.\nRepresenting patients’ status using time-series data to better anticipate clinical deterioration in the Intensive Care Unit.\nExploiting structure in medical knowledge to improve rare disease recognition during automated clinical coding.\nAn open-source software library (in Python) for preprocessing and analysis of physiological waveforms."
  },
  {
    "objectID": "people/academic/catling_finn.html#biography",
    "href": "people/academic/catling_finn.html#biography",
    "title": "Finn Catling",
    "section": "",
    "text": "I am an academic Anaesthetic and Critical Care doctor, and a Wellcome Trust 4i Clinical PhD Fellow at Imperial College London.\nMy research focuses on early diagnosis and improved treatment of critical illness, using methods from Bayesian statistics and machine learning. I am particularly interested in merging these methods with physiological models to provide bedside decision support, in disease heterogeneity, and in the role of uncertainty in clinical decision-making.\nMy recent work includes:\n\nUncertainty-aware mortality risk prediction for patients undergoing emergency laparotomy. Our prediction model is available as a web app and API.\nRepresenting patients’ status using time-series data to better anticipate clinical deterioration in the Intensive Care Unit.\nExploiting structure in medical knowledge to improve rare disease recognition during automated clinical coding.\nAn open-source software library (in Python) for preprocessing and analysis of physiological waveforms."
  },
  {
    "objectID": "people/alumni/wong_danny.html",
    "href": "people/alumni/wong_danny.html",
    "title": "Danny Wong",
    "section": "",
    "text": "Consultant Anaesthetist, Guys & St Thomas’ NHS Foundation Trust\nPhD at National Institute of Academic Anaesthesia Health Services Research Centre, Royal College of Anaesthetists & UCL\nPersonal blog"
  },
  {
    "objectID": "people/clinical/campbell_ruaraidh.html",
    "href": "people/clinical/campbell_ruaraidh.html",
    "title": "Ruaraidh Campbell",
    "section": "",
    "text": "I am a FY3-level junior doctor currently working as a Clinical Fellow in Data Science and Critical Care at UCLH. I have a clinical interest in Intensive Care Medicine, while my research interests lie within epidemiology and data science. Currently working on the Data Clinic, an initiative to improve access to electronic health record data for frontline healthcare staff.\nScottish at heart and in appearance, but enjoying the better weather down South!"
  },
  {
    "objectID": "people/clinical/campbell_ruaraidh.html#biography",
    "href": "people/clinical/campbell_ruaraidh.html#biography",
    "title": "Ruaraidh Campbell",
    "section": "",
    "text": "I am a FY3-level junior doctor currently working as a Clinical Fellow in Data Science and Critical Care at UCLH. I have a clinical interest in Intensive Care Medicine, while my research interests lie within epidemiology and data science. Currently working on the Data Clinic, an initiative to improve access to electronic health record data for frontline healthcare staff.\nScottish at heart and in appearance, but enjoying the better weather down South!"
  },
  {
    "objectID": "people/clinical/vaughan_sarah.html",
    "href": "people/clinical/vaughan_sarah.html",
    "title": "Sarah Vaughan",
    "section": "",
    "text": "I am an SHO working as a clinical research fellow in data science and critical care at UCLH. I graduated from King’s College London in 2019, having completed my pre-clinical training at University of Cambridge and obtained a BA in Physiology, Development and Neuroscience. As an undergraduate I carried out a research project investigating the effects of visual crowding.\nI have an interest in critical care and hope to enter anaesthetic training in the future. Currently, I’m working on ways to model ‘ward strain’ and using this to predict patient deterioration, including ICU admission."
  },
  {
    "objectID": "people/clinical/vaughan_sarah.html#biography",
    "href": "people/clinical/vaughan_sarah.html#biography",
    "title": "Sarah Vaughan",
    "section": "",
    "text": "I am an SHO working as a clinical research fellow in data science and critical care at UCLH. I graduated from King’s College London in 2019, having completed my pre-clinical training at University of Cambridge and obtained a BA in Physiology, Development and Neuroscience. As an undergraduate I carried out a research project investigating the effects of visual crowding.\nI have an interest in critical care and hope to enter anaesthetic training in the future. Currently, I’m working on ways to model ‘ward strain’ and using this to predict patient deterioration, including ICU admission."
  },
  {
    "objectID": "log.html",
    "href": "log.html",
    "title": "UCL Health Algorithms Laboratory",
    "section": "",
    "text": "converting to Quarto"
  },
  {
    "objectID": "log.html#section",
    "href": "log.html#section",
    "title": "UCL Health Algorithms Laboratory",
    "section": "",
    "text": "converting to Quarto"
  },
  {
    "objectID": "log.html#section-1",
    "href": "log.html#section-1",
    "title": "UCL Health Algorithms Laboratory",
    "section": "2021-12-20",
    "text": "2021-12-20\nRunning log of steps used Each time you do some work enter the date on a new line Then one line per statement thereafter https://python-poetry.org/docs/basic-usage/\nset up poetry\npoetry new uclhal\npoetry add nikola\npoetry shell\ngit\ngit init\ngit branch -m main\nhttps://getnikola.com/getting-started.html conflicts if you run the set up command from the root of the poetry project maybe something to do with their being a toml file there already? instead I did the install from a clean (naked) subdirectory site at ./uclhal/uclhal\n?prob will need to publish the site from that subdirectoy if I wish to use github"
  },
  {
    "objectID": "log.html#section-2",
    "href": "log.html#section-2",
    "title": "UCL Health Algorithms Laboratory",
    "section": "2021-12-21",
    "text": "2021-12-21\nset up pylsp in sublime\nupdated pyproject.toml\n[tool.poetry.dev-dependencies]\npytest = \"^5.2\"\npython-lsp-server = \"^1.3.1\"\npython-lsp-black = \"^1.0.0\"\nmypy-ls = \"^0.5.1\"\npyls-isort = \"^0.2.2\"\nthen\npoetry update\nthen copied settings from dashRep into uclhal.sublime-project used which pylsp to find the path to pylsp\n    \"settings\": {\n        \"LSP\": {\n            \"pylsp\": {\n                \"enabled\": true,\n                \"command\": [\n                    \"/Users/steve/Library/Caches/pypoetry/virtualenvs/uclhal-XMZ_bNmG-py3.9/bin/pylsp\",\n                ],\n                \"settings\": {\n                    \"pylsp.plugins.flake8.executable\": \"/Users/steve/Library/Caches/pypoetry/virtualenvs/uclhal-XMZ_bNmG-py3.9/bin/flake8\",\n                },\n            },\n        },\n    },\ndeploy to github uses the following tool https://github.com/c-w/ghp-import\n\nInside your repository just run ghp-import $DOCS_DIR where $DOCS_DIR is the path to the built documentation. This will write a commit to your gh-pages branch with the current documents in it.\n\nto build\ncd ./uclhal/uclhal\nnikola build\nto deploy\ncd ./uclhal/uclhal\nnikola github_deploy\n2021-12-21t11:02:53 Woo hoo! Site up, running and deployed!"
  },
  {
    "objectID": "log.html#section-3",
    "href": "log.html#section-3",
    "title": "UCL Health Algorithms Laboratory",
    "section": "2021-12-22",
    "text": "2021-12-22\nLook into github actions for automatic deploy I don’t think it will work b/c the blog lives in a subdir hence made a symlink then edited the github actions file at https://github.com/docsteveharris/nikola-action/releases/tag/v3.1.0-alpha\n… I think in the end this might have worked But was confused b/c failed b/c could not find path src but maybe b/c I was working from a feature branch So abandoned (?prematurely) And moved all blog files to top level and reset to the default github actions yml file rather than my fork this worked only when I pushed from the src branch\nbut staying with this approach ’cos seems cleaner anyway"
  },
  {
    "objectID": "log.html#section-4",
    "href": "log.html#section-4",
    "title": "UCL Health Algorithms Laboratory",
    "section": "2021-12-29",
    "text": "2021-12-29\njust getting my bearings back serving from port 8000 from command line\nnikola build && nikola serve\nuseful pages themes https://themes.getnikola.com/v8/zen/ restructured text https://docutils.sourceforge.io/docs/ref/rst/restructuredtext.html#indirect-hyperlink-targets nikola https://getnikola.com/handbook.html#indexes\nothers work https://paulomarconi.github.io/blog/Nikola-guide/ http://louistiao.me/posts/how-i-customized-my-nikola-powered-site/ https://www.brainsorting.dev/posts/create-a-blog-with-nikola/ http://hannahbarton.org.uk/pages/handbook/\nold websites https://github.com/docsteveharris/harris-data-lab/blob/main/content/authors/finn-catling/_index.md\ncanterville images served from files/images using url images/name_of_image e.g. placed the blog log there\nlots of fiddling with the logo now only displays correctly on the landing page tried to set up separate pages for each lab member prob easier? just to introduce each person via blog post with an appropriate category? then use the category to index individuals"
  },
  {
    "objectID": "log.html#section-5",
    "href": "log.html#section-5",
    "title": "UCL Health Algorithms Laboratory",
    "section": "2022-01-04",
    "text": "2022-01-04\njust added a bunch of pages for each lab member still do the HYLODE team"
  },
  {
    "objectID": "tutorials/onboarding/onboarding-2.html",
    "href": "tutorials/onboarding/onboarding-2.html",
    "title": "Onboarding 2",
    "section": "",
    "text": "You are going to create your user own user page, and publish that to the HySchool website.\n\n\nYou have completed Lesson 1 (see Lesson 1)\n\n\n\n\nSet-up\nInstall Quarto (our documentation and website system) &lt;-- you are here\nWrite and publish your first page\n\nPlease note that this readme.md file will change as we progress through the lessons. You will need to come back here after each step.",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Onboarding 2"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding-2.html#aim",
    "href": "tutorials/onboarding/onboarding-2.html#aim",
    "title": "Onboarding 2",
    "section": "",
    "text": "You are going to create your user own user page, and publish that to the HySchool website.\n\n\nYou have completed Lesson 1 (see Lesson 1)\n\n\n\n\nSet-up\nInstall Quarto (our documentation and website system) &lt;-- you are here\nWrite and publish your first page\n\nPlease note that this readme.md file will change as we progress through the lessons. You will need to come back here after each step.",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Onboarding 2"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding-2.html#lesson-2-install-quarto",
    "href": "tutorials/onboarding/onboarding-2.html#lesson-2-install-quarto",
    "title": "Onboarding 2",
    "section": "Lesson 2: Install Quarto",
    "text": "Lesson 2: Install Quarto\nFirst\n\nconfirm you know where your repo is Repository &gt; Show in Finder on your machine\nopen VS Code and go to File &gt; Open Folder… (or equivalent on Windows) and open the folder above\n\nThis lesson will show you how to get the HySchool website running on your own machine. To do this we need to run through the Quarto set-up process.\nGo to Quarto and follow the Get Started instructions. We’re going to assume you’re using the VS Code getting started tutorial.\nYou will also need to\n\ncheck you having a working Python installation\nSet-up a Python (or Conda) virtual environment (Optional but extra credit)\ninstall several extensions for VS Code including\n\nthe Quarto extension\nthe Python extension\n\n\n\nDetour (possibly important)\n\nInstalling Python and Anaconda or the DIY Miniconda version\nWorking with Python in VS Code",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Onboarding 2"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding-2.html#quarto-hello-world",
    "href": "tutorials/onboarding/onboarding-2.html#quarto-hello-world",
    "title": "Onboarding 2",
    "section": "Quarto ‘Hello World’",
    "text": "Quarto ‘Hello World’\nGo to your shell and install the necessary Python packages. They may already be there if you installed Anaconda.\npython3 -m pip install jupyter matplotlib plotly_express\nIf you run into trouble you may need to quit and re-open VSCode and the Terminal to ensure that everything is properly configured.\nYou must use python &gt; select interpreter in VSCode (see https://code.visualstudio.com/docs/python/environments#_select-and-activate-an-environment) else it will continue to default to the base setting and not run the Quarto preview etc commands using the right Python interpreter.",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Onboarding 2"
    ]
  },
  {
    "objectID": "tutorials/data-clinic/data-clinic-for-fellows.html",
    "href": "tutorials/data-clinic/data-clinic-for-fellows.html",
    "title": "Data Clinic: Onboarding for Data Fellows",
    "section": "",
    "text": "What is the ‘Data Clinic’?\nThe Data Clinic is a Quality Improvement initiative to help frontline healthcare workers in Critical Care get data from EPIC.\nThis is to improve the ease and speed of audits, QIPs and service evaluations we perform in Critical Care.\n\n\nWhat are the issues that the Data Clinic is trying to solve?\nUCLH has invested millons of pounds to obtain an fully electronic health record system (EHRS) in the form of EPIC since 2019. It aims to be a leader in the field of technologically-enabled healthcare.\nThe current method for obtaining data from EPIC is through contacting the Buisness Intelligence (BI) team, whose role it is to monitor performance data in their department and service data requests. This has a number of issues:\n\n\nMany clinical staff are unaware of who the BI team are or what they do\nThe current process of requesting data often takes the form of numerous back-and-forth emails between clinical staff and the BI team, clarifying what data is required vs what is possible to extract. This process is often time-consuming and frustrating for both the clinical staff and BI team.\nAlthough the BI team have a good understanding of how to find data in Epic and manage it, sometimes they lack the clinical understanding and direction to provide learning from it.\n\n\nThe Data Clinic aims to improve the process of providing data to clinical staff using two approaches:\n\n\nWe aim to understand the clinical question the requestor is trying to answer and, with our knowledge of the data that is available, clarify exactly what data is required for their project.\nThen we work with the BI team to provide that data for the requestor as quickly as possible, in the form that the requestor needs.\n\n\nWe hypothesise that this shall benefit both clinical staff and the BI team.\n\nClinical staff:\n- makes the process of getting data from EPIC clearer\n- enhances understanding of the data that is available from EPIC\n- help with analysis of data (if required)\n- overall makes it easier to get the data needed to complete audit/QIP/service evaluations (thereby increasing the number of projects completed and improving patient care)\n\n\nBI team:\n- Saves time/frustration emailing back-and-forth with clinical staff\n- Better understanding of clinical relevance of their work (through feedback on the results of QI projects)\n\n\n\n(Current) Structure of the Data Clinic\nBelow is a brief, introductory overview of the current stucture of the Data Clinic. A more in-depth overview can be found in the Data Clinic Teams channel.\nRequests for data can currently come from 2 channels:\n\nA referral from the Audit/QI clinic (where a project has indicated that they need EPIC data to complete their project)\nA direct email to the Data Clinic email address or one of the members of the Data Clinic team.\n\nAs a rule of thumb, all audit/QIP should go through the Audit/QI clinic prior to being referred to the Data Clinic. This allows the Audit/QIP team to register the project, keep track of it and provide methodological support prior to the Data Clinic. Other data requests that are not audit/QI (e.g. service evaluation) are more likely to come through the second route.\nOnce a request has been recieved, a Data Fellow meets with the requestor to discuss their project and the data requirements.\nFollowing this, the Data Fellow shall meet with the BI team to perform a 1st extraction of the data. This shall be shown to the requestor.\nIf they are happy with this data, we then proceed to a full data extraction and delivery. If they are not satisfied, the extraction is revised and further meeting are arranged with the requestor until the data provided is adaquate (see the below flowchart for a graphical representation of this process)\n\n\n\n\n\nflowchart TD\nA(Data request recieved) --&gt; B(Screened by Data Clinic team)\nB --&gt; C(Meeting with Data Clinic team)\nB --&gt; K(Can be served by existing report)\nC --&gt; D[1st extraction of data]\nD --&gt; F{2nd meeting between requestor & Data Clinic team}\nF --&gt; G[Happy with data]\nF --&gt; H[Revisions needed]\nG --&gt; I(Full extraction & data delivered)\nH --&gt; J(Further revision to extraction)\nJ --&gt; F\nK --&gt; L(Data delivered)\n\nlinkStyle default stroke: black;\n\n\n\n\n\n\nAs the Data Clinic is being run as a QIP, this process is subject to change over time.\n\n\nRelevant People\n\nData Clinic\nDr Tim Bonnici: Data Lead for Critical Care (t.bonnici@nhs.net)\nData Fellows: Change yearly, previous fellows are listed below\n\n2021/22: Conor Foley, Dan Stein\n2022/23: Ruaraidh Campbell, Sarah Vaughan, Hrisheekesh Vaidya\n\n\n\nBI team\nTracey Crissell: Critical BI team lead (traceycrissell@nhs.net)\nHumayra Chowdhury: BI analyst (humayra.chowdhury@nhs.net)\nThe Data Clinic also works closely with the Audit/QI team and the Methodology Clinic team\n\n\nAudit/QI team\nVeronica (‘Ronnie’) Marsh: Audit/QI lead (veronica.marsh1@nhs.net)\n\n\nMethodology team\nSiri Steinmo: Quality Improvement Lead (siri.steinmo@nhs.net)\nPetra Voegele: PERRT nurse, Methodology clinic co-lead (petra.voegele@nhs.net)\n\n\n\nOnboarding information\nThis page will have given you a good general oversight of the Data Clinic and the people involved.\nHowever, if you are going to be working with the Data Clinic team, then you shall require some further information/orientation.\nBelow is a checklist of items to arrange to get up-and-running with the Data Clinic:\n\nObtain access to the Data Clinic Teams channel (contains more detailed information of structure of Data clinic, training on how to extract data from EPIC, a tracker for projects currently going through Data Clinic, records of previous projects)\n\nEmail Tim Bonicci to obtain\n\nRead through the comprehensive “Data Clinic Handover Document” & watch the video tour of the Data Clinic Teams channel\n\nObtain access to the Data Clinic GitHub repository (contains code for extracting data from EPIC)\n\nEmail Tim Bonicci to obtain\n\nObtain access to Caboodle (EPIC’s main reporting database). Allows you to extract EPIC data.\n\nEmail David Thomson (Information Services Manager) to obtain (david.thompson19@nhs.net). You shall need to attend a mandatory training session with David before being given access\n\nComplete Caboodle training via EpicUserWeb\n\nTo sign up for an EpicUserWeb account, visit https://userweb.epic.com/Account/Register\n\nThere is a lot of training resources for using EPIC on EpicUserWeb, most of which is not relevant to the Data Clinic. The key learning to look at can be found in the “Recommended Reading List.docx” file, within the EPIC training folder in the Data Clinic Teams channel\n\nComplete SQL learning (SQL is the coding language used for extracting data from reporting databases, such as Caboodle)\n\nThere are a number of free vs paid learning reserouces for this.\nPrevious Data Fellows have bought a subscription to DataCamp. This is paid for, however it is good for learning and also has other coding languages (e.g. python, R)\n\nThere are free learning resources in the “Data Science learning resources” folder within the Data Clinic Teams channel\n\nIntroduce yourself to Ronnie, Siri & Petra from the Audit/QI and Methodology teams\n\nYou will work closely with them throughout the year. It is good to meet up with them so they can show you what they do in the Audit/QI & Methodology clinics and how this relates to the Data Clinic\n\nGet access to the Data Clinic email inbox\n\nEmail Tim Bonnici to obtain\n\n\n\n\nCurrent Progress of Project\n2021/22\n\n\nStart of project\n\nBackground to project including issues we hoped to address laid out in project overview document (see Notion document on Teams channel)\n\nSurvey done on 33 staff members in Critical Care that identified barriers to using data to complete audit/QI projects\n\n7/8 projects served through Data Clinic, including audit/QI/project feasibility projects.\n\nDraft project tracker set up to record length of time taken to serve data for data requests\n\nInformal qualitative feedback on the Data Clinic process gathered from joint meetings between Data lead/Data Fellows/BI team\n\nAdjustments made to running of the Data Clinic from this feedback including identifying the need for Audit/Methodology support for some projects before they attend Data Clinic, to clarify the aims and scope of projects.\n\nOverall, provided qualitative evidence that the Data Clinic initiative was feasible, beneficial to BI team and could help clinical staff.\n\n\n2022/23\n\n\nPlan for the year was to formalise outcome/process/balancing measurements for the Data Clinic & establish a reliable service.\n\nData Clinic email address and Teams channel set up, to create central repositiory for Data Clinic items. Github set up for code repository.\n\nAudit/QI clinic & Methodology clinic set up. Process of referring projects from these clinics to Data Clinic set up.",
    "crumbs": [
      "Tutorials",
      "Data Clinic",
      "Data Clinic: Onboarding for Data Fellows"
    ]
  },
  {
    "objectID": "tutorials/emap/emap-star-intro.html",
    "href": "tutorials/emap/emap-star-intro.html",
    "title": "Introduction to EMAP star",
    "section": "",
    "text": "A template JupyterNotebook for working with EMAP. The following features of this notebook, and associated files are documented here to minimise the risk of data leaks or other incidents.\n\nUsernames and passwords are stored in a .env file that is excluded from version control. The example env file at ./config/env should be edited and saved as ./config/.env. A utility function load_env_vars() is provided that will confirm this file exists and load the configuration into the working environment.\n.gitattributes are set to strip JupyterNotebook cells when pushing to GitHub\n\n\n\nLoad libraries\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom sqlalchemy import create_engine\n\n\nfrom utils.setup import load_env_vars\n\n\n\n\nLoad environment variables and set-up SQLAlchemy connection engine for the EMAP Star\n\n# Load environment variables\nload_env_vars()\n\n# Construct the PostgreSQL connection\nuds_host = os.getenv('EMAP_DB_HOST')\nuds_name = os.getenv('EMAP_DB_NAME')\nuds_port = os.getenv('EMAP_DB_PORT')\nuds_user = os.getenv('EMAP_DB_USER')\nuds_passwd = os.getenv('EMAP_DB_PASSWORD')\n\nemapdb_engine = create_engine(f'postgresql://{uds_user}:{uds_passwd}@{uds_host}:{uds_port}/{uds_name}')\n\nThe above code is also abstracted into a function (below) but shown in long form above to make clear what we are doing.\nfrom utils.setup import make_emap_engine\nemapdb_engine = make_emap_engine\n\n\n\nNow use the connection to work with EMAP.\nFor example, let’s inspect patients currently in ED or Resus.\nHere’s the SQL:\n-- Example script \n-- to pick out patients currently in A&E resus or majors\n\nSELECT\n   vd.location_visit_id\n  ,vd.hospital_visit_id\n  ,vd.location_id\n  -- ugly HL7 location string \n  ,lo.location_string\n  -- time admitted to that bed/theatre/scan etc.\n  ,vd.admission_time\n  -- time discharged from that bed\n  ,vd.discharge_time\n\nFROM star.location_visit vd\n-- location label\nINNER JOIN star.location lo ON vd.location_id = lo.location_id\nWHERE \n-- last few hours\nvd.admission_time &gt; NOW() - '12 HOURS'::INTERVAL    \n-- just CURRENT patients\nAND\nvd.discharge_time IS NULL\n-- filter out just ED and Resus or Majors\nAND\n-- unpacking the HL7 string formatted as \n-- Department^Ward^Bed string\nSPLIT_PART(lo.location_string,'^',1) = 'ED'\nAND\nSPLIT_PART(lo.location_string,'^',2) ~ '(RESUS|MAJORS)'\n-- sort\nORDER BY lo.location_string\n;\nThe SQL script is stored at ../snippets/sql-vignettes/current_bed.sql.\nWe can load the script, and read the results into a Pandas dataframe.\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/current_bed.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\n\ndf.head()\n\n\n\n\nA series of three scripts\n\nSimply pull hospital visits\nAdd in hospital numbers (MRN) and handle patient merges\nAdd in patient demographics\n\n\n\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  -- admission to hospital\n  ,vo.admission_time\n  ,vo.arrival_method\n  ,vo.presentation_time\n  -- discharge from hospital\n  -- NB: Outpatients have admission events but not discharge events\n  ,vo.discharge_time\n  ,vo.discharge_disposition\n\n-- start from hospital visits\nFROM star.hospital_visit vo\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_time &gt; NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_time DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/hospital_visit_1.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\nSee the series of joins in the middle of the script that retrieve the live MRN. That is we recognise that patients may have had an episode of care with one MRN, and then that episode was merged with another historical MRN. One of those two MRNs will then become the ‘live’ MRN and can be used to trace the patient across what otherwise would be different identities.\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  ,vo.admission_time\n  ,vo.arrival_method\n  ,vo.presentation_time\n  ,vo.discharge_time\n  ,vo.discharge_disposition\n  -- original MRN\n  ,original_mrn.mrn AS original_mrn\n  -- live MRN\n  ,live_mrn.mrn AS live_mrn\n\n-- start from hospital visits\nFROM star.hospital_visit vo\n-- get original mrn\nINNER JOIN star.mrn original_mrn ON vo.mrn_id = original_mrn.mrn_id\n-- get mrn to live mapping \nINNER JOIN star.mrn_to_live mtl ON vo.mrn_id = mtl.mrn_id \n-- get live mrn \nINNER JOIN star.mrn live_mrn ON mtl.live_mrn_id = live_mrn.mrn_id \n\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_time &gt; NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_time DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/hospital_visit_2.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  ,vo.admission_time\n  ,vo.arrival_method\n  ,vo.presentation_time\n  ,vo.discharge_time\n  ,vo.discharge_disposition\n  -- original MRN\n  ,original_mrn.mrn AS original_mrn\n  -- live MRN\n  ,live_mrn.mrn AS live_mrn\n\n  -- core demographics\n  ,cd.date_of_birth\n  -- convert dob to age in years\n  ,date_part('year', AGE(cd.date_of_birth)) AS age\n  ,cd.sex\n  ,cd.home_postcode\n  -- grab initials from first and last name\n  ,CONCAT(LEFT(cd.firstname, 1), LEFT(cd.lastname, 1)) AS initials\n\n-- start from hospital visits\nFROM star.hospital_visit vo\nINNER JOIN star.core_demographic cd ON vo.mrn_id = cd.mrn_id\n\n-- get original mrn\nINNER JOIN star.mrn original_mrn ON vo.mrn_id = original_mrn.mrn_id\n-- get mrn to live mapping \nINNER JOIN star.mrn_to_live mtl ON vo.mrn_id = mtl.mrn_id \n-- get live mrn \nINNER JOIN star.mrn live_mrn ON mtl.live_mrn_id = live_mrn.mrn_id \n\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_time &gt; NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_time DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/hospital_visit_3.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\n\nWIP: example script to work with vitals\n-- Example script showing how to work with observations\n\n-- V simple view that finds recent observations \n-- for current inpatients in the last few minutes\n\n\nSELECT\n  -- observation details\n   ob.visit_observation_id\n  ,ob.hospital_visit_id\n  ,ob.observation_datetime\n\n  --,ob.visit_observation_type_id\n  --,ot.id_in_application\n\n  -- label nicely\n  ,CASE \n    WHEN ot.id_in_application = '10' THEN 'SpO2'\n    WHEN ot.id_in_application = '5' THEN 'BP'\n    WHEN ot.id_in_application = '3040109304' THEN 'Oxygen'\n    WHEN ot.id_in_application = '6' THEN 'Temp'\n    WHEN ot.id_in_application = '8' THEN 'Pulse'\n    WHEN ot.id_in_application = '9' THEN 'Resp'\n    WHEN ot.id_in_application = '6466' THEN 'AVPU'\n\n  END AS vital\n\n  ,ob.value_as_real\n  ,ob.value_as_text\n  ,ob.unit \n  \nFROM\n  star.visit_observation ob\n-- observation look-up\nLEFT JOIN\n  star.visit_observation_type ot\n  on ob.visit_observation_type_id = ot.visit_observation_type_id\n\nWHERE\nob.observation_datetime &gt; NOW() - '5 MINS'::INTERVAL    \nAND\not.id_in_application in \n\n  (\n  '10'            --'SpO2'                  -- 602063230\n  ,'5'            --'BP'                    -- 602063234\n  ,'3040109304'   --'Room Air or Oxygen'    -- 602063268\n  ,'6'            --'Temp'                  -- 602063248\n  ,'8'            --'Pulse'                 -- 602063237\n  ,'9'            --'Resp'                  -- 602063257\n  ,'6466'         -- Level of consciousness\n)\nORDER BY ob.observation_datetime DESC\n;\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/vital_signs.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\nNow let’s drill down on just heart rate\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/heart_rate.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\nimport plotly.express as px\nfigx = px.histogram(df, \n                    x='value_as_real',\n                    title='Heart rate distribution at UCLH in the last 24 hours',\n                   labels={'value_as_real': 'Heart Rate'})\nfigx.show()",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Introduction to EMAP star"
    ]
  },
  {
    "objectID": "tutorials/emap/emap-star-intro.html#basic-set-up",
    "href": "tutorials/emap/emap-star-intro.html#basic-set-up",
    "title": "Introduction to EMAP star",
    "section": "",
    "text": "Load libraries\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom sqlalchemy import create_engine\n\n\nfrom utils.setup import load_env_vars",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Introduction to EMAP star"
    ]
  },
  {
    "objectID": "tutorials/emap/emap-star-intro.html#load-environment-variables",
    "href": "tutorials/emap/emap-star-intro.html#load-environment-variables",
    "title": "Introduction to EMAP star",
    "section": "",
    "text": "Load environment variables and set-up SQLAlchemy connection engine for the EMAP Star\n\n# Load environment variables\nload_env_vars()\n\n# Construct the PostgreSQL connection\nuds_host = os.getenv('EMAP_DB_HOST')\nuds_name = os.getenv('EMAP_DB_NAME')\nuds_port = os.getenv('EMAP_DB_PORT')\nuds_user = os.getenv('EMAP_DB_USER')\nuds_passwd = os.getenv('EMAP_DB_PASSWORD')\n\nemapdb_engine = create_engine(f'postgresql://{uds_user}:{uds_passwd}@{uds_host}:{uds_port}/{uds_name}')\n\nThe above code is also abstracted into a function (below) but shown in long form above to make clear what we are doing.\nfrom utils.setup import make_emap_engine\nemapdb_engine = make_emap_engine",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Introduction to EMAP star"
    ]
  },
  {
    "objectID": "tutorials/emap/emap-star-intro.html#a-first-example-script",
    "href": "tutorials/emap/emap-star-intro.html#a-first-example-script",
    "title": "Introduction to EMAP star",
    "section": "",
    "text": "Now use the connection to work with EMAP.\nFor example, let’s inspect patients currently in ED or Resus.\nHere’s the SQL:\n-- Example script \n-- to pick out patients currently in A&E resus or majors\n\nSELECT\n   vd.location_visit_id\n  ,vd.hospital_visit_id\n  ,vd.location_id\n  -- ugly HL7 location string \n  ,lo.location_string\n  -- time admitted to that bed/theatre/scan etc.\n  ,vd.admission_time\n  -- time discharged from that bed\n  ,vd.discharge_time\n\nFROM star.location_visit vd\n-- location label\nINNER JOIN star.location lo ON vd.location_id = lo.location_id\nWHERE \n-- last few hours\nvd.admission_time &gt; NOW() - '12 HOURS'::INTERVAL    \n-- just CURRENT patients\nAND\nvd.discharge_time IS NULL\n-- filter out just ED and Resus or Majors\nAND\n-- unpacking the HL7 string formatted as \n-- Department^Ward^Bed string\nSPLIT_PART(lo.location_string,'^',1) = 'ED'\nAND\nSPLIT_PART(lo.location_string,'^',2) ~ '(RESUS|MAJORS)'\n-- sort\nORDER BY lo.location_string\n;\nThe SQL script is stored at ../snippets/sql-vignettes/current_bed.sql.\nWe can load the script, and read the results into a Pandas dataframe.\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/current_bed.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\n\ndf.head()",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Introduction to EMAP star"
    ]
  },
  {
    "objectID": "tutorials/emap/emap-star-intro.html#working-with-hospital-visits",
    "href": "tutorials/emap/emap-star-intro.html#working-with-hospital-visits",
    "title": "Introduction to EMAP star",
    "section": "",
    "text": "A series of three scripts\n\nSimply pull hospital visits\nAdd in hospital numbers (MRN) and handle patient merges\nAdd in patient demographics\n\n\n\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  -- admission to hospital\n  ,vo.admission_time\n  ,vo.arrival_method\n  ,vo.presentation_time\n  -- discharge from hospital\n  -- NB: Outpatients have admission events but not discharge events\n  ,vo.discharge_time\n  ,vo.discharge_disposition\n\n-- start from hospital visits\nFROM star.hospital_visit vo\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_time &gt; NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_time DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/hospital_visit_1.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\nSee the series of joins in the middle of the script that retrieve the live MRN. That is we recognise that patients may have had an episode of care with one MRN, and then that episode was merged with another historical MRN. One of those two MRNs will then become the ‘live’ MRN and can be used to trace the patient across what otherwise would be different identities.\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  ,vo.admission_time\n  ,vo.arrival_method\n  ,vo.presentation_time\n  ,vo.discharge_time\n  ,vo.discharge_disposition\n  -- original MRN\n  ,original_mrn.mrn AS original_mrn\n  -- live MRN\n  ,live_mrn.mrn AS live_mrn\n\n-- start from hospital visits\nFROM star.hospital_visit vo\n-- get original mrn\nINNER JOIN star.mrn original_mrn ON vo.mrn_id = original_mrn.mrn_id\n-- get mrn to live mapping \nINNER JOIN star.mrn_to_live mtl ON vo.mrn_id = mtl.mrn_id \n-- get live mrn \nINNER JOIN star.mrn live_mrn ON mtl.live_mrn_id = live_mrn.mrn_id \n\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_time &gt; NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_time DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/hospital_visit_2.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\n\n\nSELECT\n   vo.hospital_visit_id\n  ,vo.encounter\n  ,vo.admission_time\n  ,vo.arrival_method\n  ,vo.presentation_time\n  ,vo.discharge_time\n  ,vo.discharge_disposition\n  -- original MRN\n  ,original_mrn.mrn AS original_mrn\n  -- live MRN\n  ,live_mrn.mrn AS live_mrn\n\n  -- core demographics\n  ,cd.date_of_birth\n  -- convert dob to age in years\n  ,date_part('year', AGE(cd.date_of_birth)) AS age\n  ,cd.sex\n  ,cd.home_postcode\n  -- grab initials from first and last name\n  ,CONCAT(LEFT(cd.firstname, 1), LEFT(cd.lastname, 1)) AS initials\n\n-- start from hospital visits\nFROM star.hospital_visit vo\nINNER JOIN star.core_demographic cd ON vo.mrn_id = cd.mrn_id\n\n-- get original mrn\nINNER JOIN star.mrn original_mrn ON vo.mrn_id = original_mrn.mrn_id\n-- get mrn to live mapping \nINNER JOIN star.mrn_to_live mtl ON vo.mrn_id = mtl.mrn_id \n-- get live mrn \nINNER JOIN star.mrn live_mrn ON mtl.live_mrn_id = live_mrn.mrn_id \n\nWHERE \n      -- hospital visits within the last 12 hours\n      vo.presentation_time &gt; NOW() - '12 HOURS'::INTERVAL   \n      -- emergencies\n  AND vo.patient_class = 'EMERGENCY'\n      -- attending via ambulance\n  AND vo.arrival_method = 'Ambulance'\n      -- sort descending\nORDER BY vo.presentation_time DESC\n; \n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/hospital_visit_3.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Introduction to EMAP star"
    ]
  },
  {
    "objectID": "tutorials/emap/emap-star-intro.html#working-with-observations",
    "href": "tutorials/emap/emap-star-intro.html#working-with-observations",
    "title": "Introduction to EMAP star",
    "section": "",
    "text": "WIP: example script to work with vitals\n-- Example script showing how to work with observations\n\n-- V simple view that finds recent observations \n-- for current inpatients in the last few minutes\n\n\nSELECT\n  -- observation details\n   ob.visit_observation_id\n  ,ob.hospital_visit_id\n  ,ob.observation_datetime\n\n  --,ob.visit_observation_type_id\n  --,ot.id_in_application\n\n  -- label nicely\n  ,CASE \n    WHEN ot.id_in_application = '10' THEN 'SpO2'\n    WHEN ot.id_in_application = '5' THEN 'BP'\n    WHEN ot.id_in_application = '3040109304' THEN 'Oxygen'\n    WHEN ot.id_in_application = '6' THEN 'Temp'\n    WHEN ot.id_in_application = '8' THEN 'Pulse'\n    WHEN ot.id_in_application = '9' THEN 'Resp'\n    WHEN ot.id_in_application = '6466' THEN 'AVPU'\n\n  END AS vital\n\n  ,ob.value_as_real\n  ,ob.value_as_text\n  ,ob.unit \n  \nFROM\n  star.visit_observation ob\n-- observation look-up\nLEFT JOIN\n  star.visit_observation_type ot\n  on ob.visit_observation_type_id = ot.visit_observation_type_id\n\nWHERE\nob.observation_datetime &gt; NOW() - '5 MINS'::INTERVAL    \nAND\not.id_in_application in \n\n  (\n  '10'            --'SpO2'                  -- 602063230\n  ,'5'            --'BP'                    -- 602063234\n  ,'3040109304'   --'Room Air or Oxygen'    -- 602063268\n  ,'6'            --'Temp'                  -- 602063248\n  ,'8'            --'Pulse'                 -- 602063237\n  ,'9'            --'Resp'                  -- 602063257\n  ,'6466'         -- Level of consciousness\n)\nORDER BY ob.observation_datetime DESC\n;\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/vital_signs.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\nNow let’s drill down on just heart rate\n\n# Read the sql file into a query 'q' and the query into a dataframe\nq = Path('../snippets/sql-vignettes/heart_rate.sql').read_text()\ndf = pd.read_sql_query(q, emapdb_engine)\n\ndf.head()\n\n\nimport plotly.express as px\nfigx = px.histogram(df, \n                    x='value_as_real',\n                    title='Heart rate distribution at UCLH in the last 24 hours',\n                   labels={'value_as_real': 'Heart Rate'})\nfigx.show()",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Introduction to EMAP star"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_0__README.html",
    "href": "tutorials/hylode/vignette_0__README.html",
    "title": "Hi (Hy?) there…",
    "section": "",
    "text": "Welcome to the Hylode platform. Thank you for making it this far.\nOver the next four notebooks, we hope to bring you from zero to sixty on why we developed the platform and how to use it.\nThe backdrop to this infastructural work is a world where translating published EHR-driven ML4H models into deployment is hard. Reseachers internationally lament this implementation gap1. Even at UCLH, flush with a new EHR and live data infrastructure in EMAP, the last mile in pushing a retrospective model requires a lot of one-off project-specific work.\nIntent on doing meaningful research and innovation in a safety critical environment - paving the way for live deployments - deployment needs to be taken more seriously. In particular, the deployment problems that the Hylode platform is looking to address include:\n\nRisk of train-deploy split, where discrepency between training and deployment code lower performance\nData drift and feature drift, where underlying shifts in the dataset harm performance unnoticed\nAd hoc ongoing model monitoring and evaluation, where continuous model scrutiny relies on ad hoc batch evaluation\n\nWe address these problem by presenting a single framework that:\na) brings together training and deployment in the same modelling workflow\nb) integrates MLOps modules addressing data drift, model evaluation etc. alongside the main platform\nRather than ad hoc algorithm deployment, Hylode is a platform for training deployable EHR models.\n\nCase to the ML4H researcher…\nDoes all this come at the expense of ease and speed for the ML researcher?\nWe hope not. Engineered into Hylode are abstractions crafted to make the specific kind of modelling EHR ML practitioners do easier, faster and more joyful.\nIn particular:\n\nHylode provides a curated set of features easily interfacing w/ other UCLH data resources such as EMAP & Caboodle. Over time, we hope the store of pre-canned features will grow, cutting the time spent on wrangling, and freeing it up for more modelling work.\nBuilt for time-series EHR data Hylode automates many of the more fiddly elements of training a time-series ML4H model. It eases splitting patient records into temporal slices (the training instances needed to feed time-series EHR models).\nEase of experimental logging & model development The Hylode system makes it easy to log results & create reproducible workflows. The same tooling then makes it straightforward to pass models over from ML teams to application developers, to then present the predictions in a clinically meaningful way.\n\nWe see these three strengths as coming together to make Hylode a central resource to develop deployable ML models at UCLH. The core driver here is that Hylode allows researchers to get their modelling done faster.\nFrom there, the deployment benefits of Hylode kick in. Starting from a single trained model, Hylode offers a seamless technical transition to running models in silent mode, and a clear path to application development and testing the real-time relevance of predictions in complex multi-disciplinary clinical settings.\n\n\nThe notebooks…\nBackground over, we can now get started on the tutorial. The basic structure of these notebooks is as follows:\n\nNotebook 0 A compact end-to-end modelling exemplar evidencing the claims above in c. 20 lines of code.\n\nFrom there, we switch to a more measured pace. The aim is to give the reader enough grounding to start working with the different system components themselves:\n\nNotebook 1 (HyCastle) More detail on feature access and preprocessing.\nNotebook 2 (HyMind) A fuller end-to-end modelling platform exemplifying how we have been using the platform so far.\nNotebook 3 (Appendix - HyFlow & HyGear) For those interested, a delve under the hood. Here you can start triangulating yourself as to how the data is pulled from EMAP.\n\n\n1. Seneviratne, M. G., Shah, N. H. & Chu, L. Bridging the implementation gap of machine learning in healthcare. Bmj Innovations 6, 45 (2020).",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Hi *(Hy?)* there..."
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_3_data_flow.html",
    "href": "tutorials/hylode/vignette_3_data_flow.html",
    "title": "Under the hood",
    "section": "",
    "text": "What follows is a quick tour under the hood of Hylode…\nIn vignette_1_training_set, we miraculously wound up with a usable set of features on running retro_dataset. This is of course because they had been lovingly built by Nel in advance.\nThis notebook is aimed as a leg-up in getting your bearings around how Hylode ingests and processes data from EMAP. I include the different pieces of code and ways of thinking that have helped me, in the hope they will help others.",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Under the hood"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_3_data_flow.html#appendix-1-running-sql-in-jupyter",
    "href": "tutorials/hylode/vignette_3_data_flow.html#appendix-1-running-sql-in-jupyter",
    "title": "Under the hood",
    "section": "Appendix 1: Running SQL in Jupyter",
    "text": "Appendix 1: Running SQL in Jupyter\n(magpied from Nels’ existing HyMind exemplar)\n\nfrom datetime import datetime, timedelta\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\nimport urllib\n\nimport arrow\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nfrom hylib.dt import LONDON_TZ, convert_dt_columns_to_london_tz\n\n\nward = 'T03'\n\n\nEMAP credentials\nEMAP credentials are allocated per user and not stored in the environment variables. You do not want your credentials to leak into the source repository.\nOne way of safeguarding is to create a file called secret at the top level of the HyMind repository (one above this notebook).\nDo this here in Jupyter and not a local copy of the repo.\nThe first line should be your UDS username and the second line should be your UDS password.\nsecret has been added to .gitignore and will be excluded from the repository.\nRead your username & password into the environment:\n\nos.environ['EMAP_DB_USER'], os.environ['EMAP_DB_PASSWORD'] = Path('../secret').read_text().strip().split('\\n')\n\n\nuds_host = os.getenv('EMAP_DB_HOST')\nuds_name = os.getenv('EMAP_DB_NAME')\nuds_port = os.getenv('EMAP_DB_PORT')\nuds_user = os.getenv('EMAP_DB_USER')\nuds_passwd = os.getenv('EMAP_DB_PASSWORD')\n\nCreate a SQLAlchemy Engine for accessing the UDS:\n\nemapdb_engine = create_engine(f'postgresql://{uds_user}:{uds_passwd}@{uds_host}:{uds_port}/{uds_name}')\n\n\nfrom hyflow.settings import SQL_DIR\nvisits_sql = (SQL_DIR / \"emap__icu_location_visit_history.sql\").read_text()\n\n\n# the point-in-time we are interested in:  7am on 17/07/2021 BST\nhorizon_dt = datetime(2021, 7, 17, 7, 0, 0).astimezone(LONDON_TZ)\n\n\nfrom hylib.load.hydef import beds_from_hydef\nbeds_df = beds_from_hydef(ward)\n\n\nvisits_df = pd.read_sql(\n    visits_sql,\n    emapdb_engine,\n    params={\"horizon_dt\": horizon_dt, \"locations\": list(beds_df.hl7_location)},\n)\n\n\nvisits_df.head()",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Under the hood"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_3_data_flow.html#appendix-2-running-some-transformer-code",
    "href": "tutorials/hylode/vignette_3_data_flow.html#appendix-2-running-some-transformer-code",
    "title": "Under the hood",
    "section": "Appendix 2: Running some Transformer code",
    "text": "Appendix 2: Running some Transformer code\n\nfrom datetime import datetime\nimport logging\n\nfrom fastapi import APIRouter\n\nfrom hylib.load.hydef import icu_observation_types_from_hydef\n\nfrom hyflow.load.icu.icu_episode_slices import icu_episode_slices_from_hyflow\nfrom hyflow.load.icu.icu_observations import icu_observations_from_hyflow\nfrom hyflow.load.icu.icu_patients import icu_patients_from_hyflow\n\nfrom hygear.transform.cog1.base import BaseCog1Transformer\nfrom typing import List\n\n\nclass AdmissionAgeTransformer(BaseCog1Transformer):\n    \"\"\"\n    An transformer for age at admission\n\n    Output Features:\n        `admission_age_years`: float\n            Patient's age in years\n    \"\"\"\n\n    input_cols = [\"episode_slice_id\", \"admission_dt\", \"dob\"]\n\n    @property\n    def output_cols(self) -&gt; List[str]:\n        return [\"episode_slice_id\", \"admission_age_years\"]\n\n    def years(self, row: pd.Series) -&gt; float:\n        if pd.isnull(row.dob):\n            return np.nan\n        else:\n            return int(row[\"admission_dt\"].year) - int(row[\"dob\"].year)\n\n    def transform(self) -&gt; pd.DataFrame:\n        output_df = self.input_df\n\n        output_df[\"admission_age_years\"] = output_df.apply(self.years, axis=1)\n\n        return output_df.loc[:, self.output_cols]\n\n\nward\n\n\nhorizon_dt = datetime(2021, 10, 12, 11, 00).astimezone(LONDON_TZ)\n\n\nepisode_slices_df = icu_episode_slices_from_hyflow(ward, horizon_dt)\n\n\nepisode_slices_df.shape\n\n\npatients_df = icu_patients_from_hyflow(\n    ward, horizon_dt, list(episode_slices_df.episode_slice_id)\n)\n\n\nage_input_df = episode_slices_df.loc[:, [\"episode_slice_id\", \"admission_dt\"]].join(\n    patients_df.loc[:, [\"episode_slice_id\", \"dob\"]].set_index(\"episode_slice_id\"),\n    on=\"episode_slice_id\",\n)\n\n\nage_df = AdmissionAgeTransformer(ward, horizon_dt, age_input_df).transform()\noutput_df = episode_slices_df.loc[:, [\"episode_slice_id\"]].join(\n    age_df.set_index(\"episode_slice_id\"), on=\"episode_slice_id\"\n)\n\n\nage_df",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Under the hood"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-Lab-example.html",
    "href": "tutorials/hylode/HyMind-Lab-example.html",
    "title": "HyMind Lab Example",
    "section": "",
    "text": "Please make a copy of this notebook and do not edit in place\nSee the system diagram for an overview of the HYLODE system components referenced in this notebook.\n(You will need to be signed into GitHub to view)",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Lab Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-Lab-example.html#packages",
    "href": "tutorials/hylode/HyMind-Lab-example.html#packages",
    "title": "HyMind Lab Example",
    "section": "Packages",
    "text": "Packages\n\nAvailable\n\nimport pkg_resources\ninstalled_packages = pkg_resources.working_set\ninstalled_packages_list = sorted([f'{i.key}=={i.version}' for i in installed_packages])\n\n\n# Uncomment to list installed packages\n# installed_packages_list\n\n\n\nNeed more?\nFor a quick installation, uncomment & run the command below (replace ujson with the package you want)\n\n# !pip install ujson\n\n\n# import ujson\n# ujson.__version__\n\nPackages installed this way will disappear when the container is restarted.\nTo have the package permanently available, please log a ticket on ZenHub requesting the package to be added to the HyMind Lab.\n\n\nSome imports\n\nfrom datetime import datetime, timedelta\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\nimport urllib\n\nimport arrow\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nfrom hylib.dt import LONDON_TZ, convert_dt_columns_to_london_tz\n\n\n\nConstants\n\nward = 'T03'",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Lab Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-Lab-example.html#emap-db",
    "href": "tutorials/hylode/HyMind-Lab-example.html#emap-db",
    "title": "HyMind Lab Example",
    "section": "EMAP DB",
    "text": "EMAP DB\nAccess to EMAP is required for multiple components of the system to function properly.\nThis includes some of the functions that are useful to run in the HyMind Lab\n\nEMAP credentials\nEMAP credentials are allocated per user and not stored in the environment variables. You do not want your credentials to leak into the source repository.\nOne way of safeguarding is to create a file called secret at the top level of the HyMind repository (next to this notebook).\nDo this here in Jupyter and not a local copy of the repo.\nThe first line should be your UDS username and the second line should be your UDS password.\nsecret has been added to .gitignore and will be excluded from the repository.\nRead your username & password into the environment:\n\nos.environ['EMAP_DB_USER'], os.environ['EMAP_DB_PASSWORD'] = Path('secret').read_text().strip().split('\\n')\n\n\nuds_host = os.getenv('EMAP_DB_HOST')\nuds_name = os.getenv('EMAP_DB_NAME')\nuds_port = os.getenv('EMAP_DB_PORT')\nuds_user = os.getenv('EMAP_DB_USER')\nuds_passwd = os.getenv('EMAP_DB_PASSWORD')\n\nCreate a SQLAlchemy Engine for accessing the UDS:\n\nemapdb_engine = create_engine(f'postgresql://{uds_user}:{uds_passwd}@{uds_host}:{uds_port}/{uds_name}')",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Lab Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-Lab-example.html#hylode-db",
    "href": "tutorials/hylode/HyMind-Lab-example.html#hylode-db",
    "title": "HyMind Lab Example",
    "section": "HYLODE DB",
    "text": "HYLODE DB\nThe hylode database is a containerised instance of Postgres 12 used by the various components to store data for use further down the pipeline.\nYou can think of it as the medium of data flow for our system.\nUnlike the uds, it is private to us.\nYou don’t need individual credentials, everthing is baked into the environment variables.\nThere are several schemas, roughly one for each subsystem (see link to system diagram above).\nStoring data in and retrieving data from the hylode database happens through the APIs provided by the hyflow, hygear & hycastle modules.\nDirect interaction with the database is not an expected part of the HyMind workflow and presented here for interest only.\n\ndb_host = os.getenv('HYLODE_DB_HOST')\ndb_name = os.getenv('HYLODE_DB_NAME')\ndb_user = os.getenv('HYLODE_DB_USER')\ndb_passwd = os.getenv('HYLODE_DB_PASSWORD')\ndb_port = os.getenv('HYLODE_DB_PORT')                                                                                                       \n\n\nhydb_engine = create_engine(f'postgresql://{db_user}:{db_passwd}@{db_host}:{db_port}/{db_name}')\n\n\nHyDef\nThe hydef schema in the hylode database contains static reference data\n\nLocations\n\nbeds_df = pd.read_sql(\n    \"\"\"\n    select \n        bed.id\n        ,bed.code\n        ,bed.source_id\n        ,bay.code\n        ,bay.type\n        ,ward.code\n        ,ward.name\n        ,ward.type\n    from\n        hydef.beds bed\n    inner join hydef.bays bay on bed.bay_id = bay.id\n    inner join hydef.wards ward on ward.code = bay.ward_code\n    order by ward.code, bay.code, bed.code\n    \"\"\",\n    hydb_engine\n)\n\n\nbeds_df\n\n\n\nICU Observation Types Catalogue\nA growing list of observation types that we are interested in for the ICU pipeline\n\nicu_obs_types = pd.read_sql('select * from hydef.icu_observation_types', hydb_engine)\n\n\nicu_obs_types",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Lab Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-Lab-example.html#retrospective-data",
    "href": "tutorials/hylode/HyMind-Lab-example.html#retrospective-data",
    "title": "HyMind Lab Example",
    "section": "Retrospective Data",
    "text": "Retrospective Data\n\ntraining_df = retro_dataset(ward)\n\n\ntraining_df.shape\n\n\ntraining_df.head()\n\n\ntraining_df.columns\n\n\ntraining_df.episode_slice_id.duplicated().any()\n\n\ntraining_df.isnull().any()",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Lab Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-Lab-example.html#live-data",
    "href": "tutorials/hylode/HyMind-Lab-example.html#live-data",
    "title": "HyMind Lab Example",
    "section": "Live Data",
    "text": "Live Data\nThese functions return personally identifiable information\n\nHyLode Live Episode Slices\n\nprediction_df = live_dataset(ward)\n\n\nprediction_df.shape\n\n\nprediction_df\n\n\n\nEMAP Live Census Snapshot\n\nemap_df = emap_snapshot(ward)\n\n\nemap_df.head()\n\n\n\nFilter\nLimit episode slices used for prediction to admissions that are in the EMAP census\n\nprediction_df.csn.isin(emap_df.csn)\n\n\nprediction_df = prediction_df[prediction_df.csn.isin(emap_df.csn)]\n\n\nprediction_df",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Lab Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-Lab-example.html#minimal-example-using-the-hyflow-package",
    "href": "tutorials/hylode/HyMind-Lab-example.html#minimal-example-using-the-hyflow-package",
    "title": "HyMind Lab Example",
    "section": "Minimal example using the HyFlow package",
    "text": "Minimal example using the HyFlow package\n\nFetch ICU Episode Slices from EMAP\n\n# the point-in-time we are interested in:  7am on 17/07/2021 BST\nhorizon_dt = datetime(2021, 7, 17, 7, 0, 0).astimezone(LONDON_TZ)\n\n\nbeds_df = beds_from_hydef(ward)\n\n\nepisode_slices_df = icu_episode_slices_from_emap(ward, horizon_dt, list(beds_df.hl7_location))\n\nThe HyFlow method adds the episode_key as that is a HYLODE concept and not available in EMAP.\n\nepisode_slices_df\n\n\n# Attach HyDef bed_id to episode slice & drop HL7 location string\nepisode_slices_df = episode_slices_df.join(\n    beds_df.loc[:, ['bed_id', 'hl7_location']].set_index('hl7_location'),\n    on='hl7_location'\n).drop(columns=['hl7_location'])\n\n\nepisode_slices_df\n\n\n\nFetch matching Patients from EMAP for Episode Slices that are in in HyFlow\n\n# the point-in-time we are interested in:  8pm on 17/07/2021 BST\nhorizon_dt = datetime(2021, 7, 17, 20, 0, 0).astimezone(LONDON_TZ)\n\n\n# get our saved Episode Slices\nepisode_slices_df = icu_episode_slices_from_hyflow(ward, horizon_dt)\n\n\nepisode_slices_df.head()\n\n\npatients_df = icu_patients_from_emap(ward, horizon_dt, list(episode_slices_df.csn))\n\n\npatients_df\n\n\n# Attach HyFlow episode_slice_id to patient\npatients_df = patients_df.join(\n    episode_slices_df.loc[:, ['episode_slice_id', 'csn']].set_index('csn'),\n    on='csn'\n).drop(columns=['csn'])\n\n\npatients_df\n\n\n\nFetch matching Observations of interest in EMAP for Episode Slices that are in HyFlow\n\nlookback_hrs = 24 # size of the trailing window we are interested in\n\n\n# the point-in-time we are interested in:  10am on 17/07/2021 BST\nhorizon_dt = datetime(2021, 7, 17, 10, 0, 0).astimezone(LONDON_TZ)\n\n\n# get our saved Episode Slices\nepisode_slices_df = icu_episode_slices_from_hyflow(ward, horizon_dt)\n\n\nepisode_slices_df.head()\n\n\n# get our reference list of observation types we care about\nobs_types_df = icu_observation_types_from_hydef()\n\n\nobs_types_df.head()\n\n\nobs_df = icu_observations_from_emap(\n    ward,\n    horizon_dt,\n    list(episode_slices_df.csn),\n    list(obs_types_df.source_id),\n    lookback_hrs\n)\n\n\nobs_df\n\n\n# Attach HyDef observation type id to observation\nobs_df = obs_df.join(\n    obs_types_df.loc[:, ['obs_type_id', 'source_id']].set_index('source_id'),\n    on='source_id'\n)\n\n\n# Attach HyFlow episode_slice_id to observation\nobs_df = obs_df.join(\n    episode_slices_df.loc[:, ['episode_slice_id', 'csn']].set_index('csn'),\n    on='csn'\n).drop(columns=['csn'])\n\n\nobs_df",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Lab Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-Lab-example.html#accessing-the-hyflow-schema-directly",
    "href": "tutorials/hylode/HyMind-Lab-example.html#accessing-the-hyflow-schema-directly",
    "title": "HyMind Lab Example",
    "section": "Accessing the hyflow schema directly",
    "text": "Accessing the hyflow schema directly\nDirectly querying the hylode database will return personally identifiable information\nLike most tables in the hylode database, the hyflow schema tables are all immutable logs.\nThat means data is only ever appended, never updated in place.\nThis also means, for example, that an individual patient will have many records in the icu_patients_log table,\none for each slice that was taken while their episode was active.\nImportant notes about direct Hylode DB access:\nThe queries provided through the functions in the hyflow & hygear packages take care of removing duplicates.\nIf you access the schemas directly you will need to do that yourself - see the various hyflow__*.sql files in hygear/load/sql for examples of partitioning over episode_slice_id and horizon_dt columns.\nOther conveniences are provided by the packages.\nFor example, the Postgres/SQLAlchemy/Pandas stack does not support storing timedeltas directly (even though it is a supported data type in both Postgres & Pandas, SQLAlchemy is unable to handle it).\nThat means the raw span column in the hyflow.icu_episode_slices_log table is in nanoseconds.\nConverting to a timedelta is done in the packages but you’ll have to do that yourself if you access the raw tables.\n\nICU Episode Slices with Bed Id\n\nsql_episode_slices_df = pd.read_sql(\n    '''\n        select \n            ep.id AS episode_slice_id\n            , ep.episode_key\n            , ep.csn\n            , ep.admission_dt\n            , ep.discharge_dt\n            , beds.source_id AS bed\n            , horizon_dt\n            , log_dt\n        from hyflow.icu_episode_slices_log ep\n            inner join hydef.beds beds ON ep.bed_id = beds.id\n            inner join hydef.bays bays ON beds.bay_id = bays.id\n         WHERE bays.ward_code = %(ward)s\n        order by episode_key, horizon_dt limit 1000\n    ''', \n    hydb_engine,\n    params={'horizon_dt': horizon_dt, 'ward': 'T03'}\n)\n\n\nsql_episode_slices_df\n\n\n\nICU Patients\n\nsql_patients_df = pd.read_sql('''\n    select \n        id AS patient_log_id\n        , episode_slice_id\n        , mrn\n        , name\n        , dob\n        , sex\n        , ethnicity\n        , postcode\n        , horizon_dt\n        , log_dt\n    from hyflow.icu_patients_log \n    order by mrn, horizon_dt limit 1000''', \n    hydb_engine\n)\n\n\nsql_patients_df\n\n\n\nICU Observations\n\nsql_obs_df = pd.read_sql('select * from hyflow.icu_observations_log order by episode_slice_id, horizon_dt limit 1000', hydb_engine)\n\n\nsql_obs_df",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Lab Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-Lab-example.html#single-horizon-example",
    "href": "tutorials/hylode/HyMind-Lab-example.html#single-horizon-example",
    "title": "HyMind Lab Example",
    "section": "Single Horizon Example",
    "text": "Single Horizon Example\n\n# the point-in-time we are interested in:  10pm on 31/08/2021 BST\nhorizon_dt = datetime(2021, 8, 31, 22, 0, 0).astimezone(LONDON_TZ)\n\n\nFetch ICU Episode Slices active at a specific point-in-time\n\nepisode_slices_df = icu_episode_slices_from_hyflow(\n    ward,\n    horizon_dt\n)\n\n\nepisode_slices_df\n\n\n\nFetch matching Patients for ICU Episode Slices active at a specific point-in-time\n\n# fetch matching patients\npatients_df = icu_patients_from_hyflow(\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\npatients_df.head()\n\n\n# join episode slices with patients\ncombined_df = episode_slices_df.join(\n        patients_df.loc[:, ['episode_slice_id', 'mrn', 'dob', 'sex', 'ethnicity']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    ).drop(['log_dt', 'horizon_dt'], axis=1)\n\n\ncombined_df.head()\n\n\n\nFetch matching Observations for ICU Episode Slices active at a specific point-in-time\nthis is in long format, multiple rows per episode_slice_id\n\n# number of trailing hours we are interested in\nlookback_hrs = 24\n\n\n# fetch matching observations\nobs_df = icu_observations_from_hyflow(\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id),\n    lookback_hrs\n)\n\n\nobs_df.head()\n\n\n# fetch the observation types reference catalogue\nobs_types_df = icu_observation_types_from_hydef()\n\n\n# join observations with metadata\nobs_df = obs_df.join(\n    obs_types_df.set_index('obs_type_id'),\n    on='obs_type_id'\n)\n\n\nobs_df.head()\n\n\n# join observations with episode slices to get episode key\neps_obs_df = obs_df.join(\n    episode_slices_df.loc[:, ['episode_slice_id', 'episode_key', 'admission_dt', 'discharge_dt']].set_index('episode_slice_id'),\n    on='episode_slice_id'\n)\n\n\neps_obs_df.groupby('episode_key')['obs_id'].count().rename('n_observations')\n\n\n\nFetch generated ICU Patient State Features for ICU Episode Slices active at a specific point-in-time\n\npatient_state_df = icu_features_from_hygear(\n    'patient_state',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\npatient_state_df\n\n\n# join with patient state\ncombined_df = combined_df.join(\n        patient_state_df.loc[:, ['episode_slice_id', 'is_proned_1_4h', 'discharge_ready_1_4h', 'is_agitated_1_8h']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Therapeutics Features for ICU Episode Slices active at a specific point-in-time\n\ntherapeutics_df = icu_features_from_hygear(\n    'therapeutics',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\ntherapeutics_df\n\n\n# join with therapeutics\ncombined_df = combined_df.join(\n        therapeutics_df.loc[:, ['episode_slice_id', 'n_inotropes_1_4h', 'had_nitric_1_8h', 'had_rrt_1_4h']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Ventilation Features for ICU Episode Slices active at a specific point-in-time\n\nventilation_df = icu_features_from_hygear(\n    'ventilation',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\nventilation_df\n\n\n# join with ventilation\ncombined_df = combined_df.join(\n        ventilation_df.loc[:, ['episode_slice_id', 'had_trache_1_12h', 'vent_type_1_4h']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Vitals Features for ICU Episode Slices active at a specific point-in-time\n\nvitals_df = icu_features_from_hygear(\n    'vitals',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\nvitals_df\n\n\n# join with vitals\ncombined_df = combined_df.join(\n        vitals_df.loc[:, ['episode_slice_id', 'avg_heart_rate_1_24h', 'max_temp_1_12h', 'avg_resp_rate_1_24h']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Work Intensity Metric Features for ICU Episode Slices active at a specific point-in-time\n\nwim_df = icu_features_from_hygear(\n    'work_intensity',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\nwim_df\n\n\n# join with work intensity\ncombined_df = combined_df.join(\n        wim_df.loc[:, ['episode_slice_id', 'wim_1']].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nFetch generated ICU Temporal Features for ICU Episode Slices active at a specific point-in-time\n\ntemporal_df = icu_features_from_hygear(\n    'temporal',\n    ward,\n    horizon_dt,\n    list(episode_slices_df.episode_slice_id)\n)\n\n\ntemporal_df\n\n\n# join with temporal\ncombined_df = combined_df.join(\n        temporal_df.loc[:, ['episode_slice_id', 'elapsed_los_td', 'total_los_td', 'remaining_los_td',]].set_index('episode_slice_id'),\n        on='episode_slice_id'\n    )\n\n\ncombined_df.head()\n\n\n\nCombined\n\ncombined_df.shape\n\n\ncombined_df.head()",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Lab Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyCastle-Lens.html",
    "href": "tutorials/hylode/HyCastle-Lens.html",
    "title": "HyCastle Lens Example",
    "section": "",
    "text": "Please make a copy of this notebook and do not edit in place\nSee the system diagram for an overview of the HYLODE system components referenced in this notebook.\n(You will need to be signed into GitHub to view)",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyCastle Lens Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyCastle-Lens.html#setup-emap-db",
    "href": "tutorials/hylode/HyCastle-Lens.html#setup-emap-db",
    "title": "HyCastle Lens Example",
    "section": "Setup EMAP DB",
    "text": "Setup EMAP DB\nAccess to EMAP is required for HyCastle to function properly.\n\nEMAP credentials\nEMAP credentials are allocated per user and not stored in the environment variables. You do not want your credentials to leak into the source repository.\nOne way of safeguarding is to create a file called secret at the top level of the HyMind repository (next to this notebook).\nDo this here in Jupyter and not a local copy of the repo.\nThe first line should be your UDS username and the second line should be your UDS password.\nsecret has been added to .gitignore and will be excluded from the repository.\nRead your username & password into the environment:\n\nos.environ['EMAP_DB_USER'], os.environ['EMAP_DB_PASSWORD'] = Path('secret').read_text().strip().split('\\n')",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyCastle Lens Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyCastle-Lens.html#retrospective-training-data",
    "href": "tutorials/hylode/HyCastle-Lens.html#retrospective-training-data",
    "title": "HyCastle Lens Example",
    "section": "Retrospective Training Data",
    "text": "Retrospective Training Data\n\ntrain_df = retro_dataset(ward)\n\n\ntrain_df.shape\n\n\ntrain_df.head()",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyCastle Lens Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyCastle-Lens.html#live-data-for-prediction",
    "href": "tutorials/hylode/HyCastle-Lens.html#live-data-for-prediction",
    "title": "HyCastle Lens Example",
    "section": "Live Data for Prediction",
    "text": "Live Data for Prediction\n\npredict_df = live_dataset(ward)\n\n\npredict_df.shape\n\n\npredict_df.head()",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyCastle Lens Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyCastle-Lens.html#example-los-model-lens",
    "href": "tutorials/hylode/HyCastle-Lens.html#example-los-model-lens",
    "title": "HyCastle Lens Example",
    "section": "Example LoS Model Lens",
    "text": "Example LoS Model Lens\n\nlens = ICUSitRepLoSLens()\n\n\nFit on Training Data\nFocus the Lens on the training dataset\n\nprocessed_train_df = lens.fit_transform(train_df)\n\n\nprocessed_train_df.shape\n\n\nprocessed_train_df.head()\n\n\nprocessed_train_df.dtypes\n\n\nprocessed_train_df.isnull().any()\n\n\n\nTransform Prediction Data\nUse the Lens that was fitted on the training dataset to transform the prediction dataset\n\npredict_df.head()\n\n\nprocessed_predict_df = lens.transform(predict_df)\n\n\nprocessed_predict_df.head()\n\n\nprocessed_predict_df.dtypes\n\n\nprocessed_predict_df.isnull().any()\n\nLive prediction dataset doesn’t have discharge_dts so total_los_td & remaining_los_td are unavailable",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyCastle Lens Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyCastle-Lens.html#sitrep-ui-lens",
    "href": "tutorials/hylode/HyCastle-Lens.html#sitrep-ui-lens",
    "title": "HyCastle Lens Example",
    "section": "SitRep UI Lens",
    "text": "SitRep UI Lens\n\nlens = ICUSitRepUiLens()\n\n\noutput_df = lens.fit_transform(predict_df)\n\n\noutput_df.head()\n\n\noutput_df.dtypes",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyCastle Lens Example"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyCastle-Lens.html#crafting-a-new-lens",
    "href": "tutorials/hylode/HyCastle-Lens.html#crafting-a-new-lens",
    "title": "HyCastle Lens Example",
    "section": "Crafting a new Lens",
    "text": "Crafting a new Lens\n\nRequired imports\n\nfrom typing import *\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import make_column_selector\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline as SKPipeline\nfrom sklearn.impute import MissingIndicator, SimpleImputer\n\nfrom hycastle.icu_store import SITREP_FEATURES\nfrom hycastle.lens.base import BaseLens\nfrom hycastle.lens.icu import *\n\n\n\nAvailable features\nThe following features are available in the ICU SitRep Pipeline\n\nSITREP_FEATURES\n\n\n\nDefinintion\n\nclass CustomICUSitRepLens(BaseLens):\n    \"\"\"\n    Lens to focus ICU SitRep data \n    \"\"\"\n    \n    # Whether to convert all columns in the output dataframe to floating point\n    numeric_output = True\n    \n    # Select a subset of SITREP_COLUMNS to include in this Lens\n    @property\n    def input_cols(self) -&gt; List[str]:\n        return [\n            'episode_slice_id',\n            'episode_key',\n            'admission_dt',\n            'bay_type',\n            'sex',\n            'ethnicity',\n            'admission_age_years',\n            'avg_heart_rate_1_24h',\n            'discharge_ready_1_4h',\n            'n_inotropes_1_4h',\n            'vent_type_1_4h',\n            'wim_1',\n            'elapsed_los_td',\n            'horizon_dt'\n        ]\n\n    specification = ColumnTransformer(\n        [\n            (\n                # Subset of columns that require no pre-processing and can be passed through as is\n                'select',\n                'passthrough',\n                [\n                    'episode_slice_id',\n                    'admission_age_years',\n                    'n_inotropes_1_4h',\n                    'wim_1',\n                ]\n            ),\n            # Pre-processing operations to apply to columns\n            # Input is 3-element tuple:\n            #   - name for pre-processing step\n            #   - transformer instance\n            #   - list of columns to apply to\n            # Any Scikit-Learn Transformer type is valid.\n            # Scikit-Learn Pipeline can also be used to combine multiple transformation for a single column, applied sequentially.\n            # The only caveat is that when a SK Pipeline is used, any transformation that creates new columns, e.g. adding a missing indicator column or\n            # one-hot-encoding, it must be the last step in the Pipeline.\n            # Details & examples for implementing custom pre-processing transformations are in `hycastle.lens.utils` \n            (\n                'admission_dt_exp',\n                DateTimeExploder(),\n                ['admission_dt']\n            ),\n            (\n                'bay_type_enc',\n                OneHotEncoder(),\n                ['bay_type']\n            ),\n            (\n                'sex_enc',\n                OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),\n                ['sex']\n            ),\n            (\n                'ethnicity_miss',\n                MissingIndicator(\n                    features='all',\n                    missing_values=None\n                ),\n                ['ethnicity']\n            ),\n            (\n                'ethnicity_enc',\n                OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),\n                ['ethnicity']\n            ),\n            (\n                'discharge_ready_1_4h_enc',\n                OneHotEncoder(handle_unknown='ignore'),\n                ['discharge_ready_1_4h']\n            ),\n            (\n                'vent_type_1_4h_enc',\n                OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1),\n                ['vent_type_1_4h']\n            ),\n            (\n                'avg_heart_rate_1_24h_prep',\n                SKPipeline(\n                    steps=[\n\n                        (\n                            'avg_heart_rate_1_24h_scale',\n                            StandardScaler(),\n                        ),\n                        (\n                            'avg_heart_rate_1_24h_impute',\n                            SimpleImputer(strategy='mean', add_indicator=True),\n                        ),\n                    ]\n                ),\n                ['avg_heart_rate_1_24h']\n            ),\n            (\n                'elapsed_los_td_seconds',\n                FunctionTransformer(timedelta_as_hours),\n                ['elapsed_los_td']\n            ),\n            (\n                'horizon_dt_exp',\n                DateTimeExploder(),\n                ['horizon_dt']\n            ),\n        ]\n    )\n\n\nApplication\n\nlens = CustomICUSitRepLens()\n\n\ntdf = lens.fit_transform(train_df)\n\n\ntdf\n\n\npdf = lens.transform(predict_df)\n\n\npdf",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyCastle Lens Example"
    ]
  },
  {
    "objectID": "news/hdruk-award-2021/index.html",
    "href": "news/hdruk-award-2021/index.html",
    "title": "HDR-UK Team of the Year Finalist",
    "section": "",
    "text": "Getting data in near real time offers profound benefits for everything from clinical decisions about individual patients to resource management across entire hospitals. A multidisciplinary team from University College London Hospitals (UCLH) spent two years developing the Experimental Medicine Application Platform (EMAP) platform to make fuller, faster, more effective use of data in electronic health records (EHRs).\nThe COVID pandemic saw them demonstrate the quality of the platform and the team. Using the EMAP infrastructure they rapidly created a bed management tool that helped take pressure off north London intensive care units (ICUs). Now the team has facilitated a series of grants to take their work into new areas including better antibiotic stewardship."
  },
  {
    "objectID": "news/hdruk-award-2021/index.html#outstanding-achievements",
    "href": "news/hdruk-award-2021/index.html#outstanding-achievements",
    "title": "HDR-UK Team of the Year Finalist",
    "section": "Outstanding achievements",
    "text": "Outstanding achievements\nWhen COVID cases surged the EMAP team, co-led by UCLH critical care consultant Dr Steve Harris, responded with a bed management tool that reported in near real time. This allowed staff to be allocated where they were most needed and kept transfer beds available at all times. The tool supported the treatment of 434 critically ill patients (including retrieving 162 patients to the UCHL surge centre) from overstretched units across north London."
  },
  {
    "objectID": "resources/homegrown.html",
    "href": "resources/homegrown.html",
    "title": "Home grown",
    "section": "",
    "text": "Home grown tools, data, and other resources.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDescription\n\n\nCategories\n\n\n\n\n\n\nData Clinic: A ‘How to’ Template\n\n\nAn overview of the Data Clinic methodology, to provide interested parties with a template for setting up their own Data Clinic\n\n\nData Clinic, Noobie, UCLH\n\n\n\n\nUCL Health Data Science MSc\n\n\n 1 year full-time taught MSc, part-time options available\n\n\nacademic, MSc, UCL\n\n\n\n\nUCL CDT in AI-enabled Healthcare Systems\n\n\n4 year MRes+PhD programme\n\n\nacademic, PhD, UCL\n\n\n\n\nNIHR Health Informatics Collaborative - Critical Care\n\n\nReal world data from NHS Critical Care Units\n\n\ndatasets, CCHIC, NHS\n\n\n\n\ncleanEHR\n\n\nReal world data from NHS Critical Care Units\n\n\nR package, CCHIC, NHS, legacy\n\n\n\n\ninspectEHR\n\n\nData Quality Evaluation tool for CCHIC\n\n\nR package, CCHIC, NHS\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Resources",
      "Home grown"
    ]
  },
  {
    "objectID": "resources/homegrown/phd_cdt_ai.html",
    "href": "resources/homegrown/phd_cdt_ai.html",
    "title": "UCL CDT in AI-enabled Healthcare Systems",
    "section": "",
    "text": "The UKRI Centre for Doctoral Training (CDT) in AI-enabled Healthcare Systems combines UCL’s excellence both in AI and computational science, and in biomedical research.\nWe offer a unique programme consisting of a 1 year MRes followed by a 3 year PhD embedded within an NHS setting.\nCurrent lab group members:\n* 2020 intake: Aasiyah Rashan, Chris Tomlinson\n* 2021 intake: Tom Frost, Jen Hunter\nPrevious CDT projects offered by the lab group:\n* 2022: Use of reinforcement learning to optimise dosing of IV infusions in critical care. (Supervisor: Steve Harris, Student: Tom Frost)\n* 2021: Hyperlocal beds plus demand forecasting within the NHS (Supervisor: Steve Harris, Student: Jen Hunter)\nApplications typically begin in January for September entry.\n* Applicants are advised to apply early, priority will be given to those who have applied in round one.\n* NB applicants with a clinical background or degree in Biomedical Science must be able to demonstrate strong computational skills, prior to applying"
  },
  {
    "objectID": "resources/homegrown/inspectehr.html",
    "href": "resources/homegrown/inspectehr.html",
    "title": "inspectEHR",
    "section": "",
    "text": "inspectEHR is a quality evaluation tool for CC-HIC. The goal of this package is to provide a core set of functions to detect erroneous or otherwise questionable data within CC-HIC, so that the end researcher can handle this information explicitly.\nThe design ethos of inspectEHR is to apply a comprehensive interpretation of the Khan data quality evaluation framework.\nThe full default evaluation is largely automated, and can be performed by use of the perform_evaluation() function. The output from this function is stored in the CC-HIC database as metadata, so that downstream research can use a consistent set of rules when faced with erroneous data patterns.\nHelper functions for extracting and wrangling data have also been developed. These live in the sister package wranglEHR, which must be installed for inspectEHR to function correctly."
  },
  {
    "objectID": "resources/homegrown/CCHIC.html",
    "href": "resources/homegrown/CCHIC.html",
    "title": "NIHR Health Informatics Collaborative - Critical Care",
    "section": "",
    "text": "Since 2014 data from the critical care units at Cambridge, Guys/Kings/St Thomas’, Imperial, Oxford, and University College London has been extracted and stored securely in a standardised format.\nThese data are crucially needed by healthcare professionals for the delivery and continuity of care; by administrators for audit, planning and service improvement; and by academic and industry researchers for the translation of scientific progress into patient benefit. Through this process, CC-HIC can improve patient outcomes, reduce the costs of care, and accelerate the pace of translational health research.\nThe physical database is held at the UCL IDHS within the Information Services Division of University College London (UCL). UCL manage and ensure that the database and the surrounding governance structures are appropriate for holding identifiable and sensitive NHS data. The safe haven is compliant to NHS Information Governance Toolkit Level 2 and operates to the ISO 27001, the Safe Haven already holds identifiable, sensitive NHS data for secondary purpose. The Critical Care HIC management group will maintain oversight and be the point of contact for researchers wishing to access data.\nThe original data specification and tooling has now been archived but can be reviewed here. The latest documentation is now here. The latest version has moved to the OMOP Common Data Model."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Research overview",
    "section": "",
    "text": "Timely delivery of critical care – We are interested in methodologically rigorous methods of evaluating the impact of delay to treatment.",
    "crumbs": [
      "Resources",
      "Research overview"
    ]
  },
  {
    "objectID": "resources.html#clinical-topics",
    "href": "resources.html#clinical-topics",
    "title": "Research overview",
    "section": "",
    "text": "Timely delivery of critical care – We are interested in methodologically rigorous methods of evaluating the impact of delay to treatment.",
    "crumbs": [
      "Resources",
      "Research overview"
    ]
  },
  {
    "objectID": "resources.html#methodological-topics",
    "href": "resources.html#methodological-topics",
    "title": "Research overview",
    "section": "Methodological Topics",
    "text": "Methodological Topics\n\nCausal Inference – We are interested in … (see BJA series and ICM paper)\nData science teaching",
    "crumbs": [
      "Resources",
      "Research overview"
    ]
  },
  {
    "objectID": "resources.html#research-funding",
    "href": "resources.html#research-funding",
    "title": "Research overview",
    "section": "Research Funding",
    "text": "Research Funding\n\nCurrent Funding\n\n“Collaborative Healthcare Innovation through Mathematics, EngineeRing and AI”\n\nEPSRC\nHarris (Co-Investigator)\n2020-\n\n\n\n\nCompleted Funding\n\n“Collaborative Healthcare Innovation through Mathematics, EngineeRing and AI”\n\nUniversity College London Hospital Charity\nBrealey D, Harris S, MacCallum N (Co-Principal Investigators):\n2017-2020\n\n\n\n–&gt;",
    "crumbs": [
      "Resources",
      "Research overview"
    ]
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "UCL Health Algorithms Laboratory",
    "section": "",
    "text": "Group Leader\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost-Doctoral Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost Doctoral Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClinical PhD Fellow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost-doctoral Researcher\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Student\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#academic-members",
    "href": "people.html#academic-members",
    "title": "UCL Health Algorithms Laboratory",
    "section": "",
    "text": "Group Leader\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost-Doctoral Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost Doctoral Researcher\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClinical PhD Fellow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost-doctoral Researcher\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Student\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPhD Student\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#clinical-members",
    "href": "people.html#clinical-members",
    "title": "UCL Health Algorithms Laboratory",
    "section": "Clinical members",
    "text": "Clinical members\n\n\n\n\n\n\n\n\n\n\nRuaraidh Campbell\n\n\nClinical Fellow in Data Science & Critical Care\n\n\n\n\nStarted\n\n\nAugust 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeddy Tun Win HLA\n\n\nClinical Fellow in Data Science & Critical Care\n\n\n\n\nStarted\n\n\nFebruary 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHrisheekesh Vaidya\n\n\nClinical Fellow in Data Science & Critical Care\n\n\n\n\nStarted\n\n\nAugust 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSarah Vaughan\n\n\nClinical Fellow in Data Science & Critical Care\n\n\n\n\nStarted\n\n\nAugust 2022\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#associate-members",
    "href": "people.html#associate-members",
    "title": "UCL Health Algorithms Laboratory",
    "section": "Associate members",
    "text": "Associate members\n\n\n\n\n\n\n\n\n\n\nDaniel Hanlon\n\n\nMedical Student\n\n\n\n\nStarted\n\n\nJune 2022\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#alumni",
    "href": "people.html#alumni",
    "title": "UCL Health Algorithms Laboratory",
    "section": "Alumni",
    "text": "Alumni\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Name\n        \n         \n          Role\n        \n         \n          Started\n        \n         \n          Ended\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nName\n\n\nRole\n\n\nStarted\n\n\nEnded\n\n\n\n\n\n\nDanny Wong\n\n\n2015-2019\n\n\n2015\n\n\n2019\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UCL Health Algorithm Laboratory",
    "section": "",
    "text": "We work on translational data science for critical care and related specialties. We bring together four interrelated disciplines within the lab:\n\nLearning Healthcare Systems via nudge randomisation\nAlgorithm stewardship and real world / real time decision support\nCollaborative data science and national level epidemiology\nNovel Non-Invasive Physiological Measurements and Assistance"
  },
  {
    "objectID": "index.html#opportunities",
    "href": "index.html#opportunities",
    "title": "UCL Health Algorithm Laboratory",
    "section": "Opportunities",
    "text": "Opportunities\n\n\n\n\n\n\nWe are currently recruiting!\n\n\n\nIf you are interested in joining the lab as a Data Scientist, please get in touch. \n\n\n\nStudents\nUndergraduate students work as research assistants, learning about the scientific method and assisting in the collection and processing of research data. Research assistants attend weekly lab meetings and can expect a letter of recommendation describing their work in and contributions to the lab (please give one month of advanced notice). Opportunities to earn co-authorship on research products (e.g., conference presentations) are also occasionally offered to advanced research assistants."
  },
  {
    "objectID": "index.html#visit-us",
    "href": "index.html#visit-us",
    "title": "UCL Health Algorithm Laboratory",
    "section": "Visit Us",
    "text": "Visit Us\nThe UCL Health Algorithm Laboratory is based at Institute of Health Informatics at University College London.\n\nStreet Address: 222 Euston Road, London, NW1 2DA"
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "UCL Health Algorithms Laboratory",
    "section": "",
    "text": "HDR-UK Team of the Year Finalist\n\n\n\n\n\n\nnews\n\n\nawards\n\n\nemap\n\n\nhdr-uk\n\n\n\nData science platform supports COVID bed management across North London Critical Care units\n\n\n\n\n\nDec 21, 2021\n\n\nSteve Harris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources/homegrown/msc_health_data_science.html",
    "href": "resources/homegrown/msc_health_data_science.html",
    "title": "UCL Health Data Science MSc",
    "section": "",
    "text": "The UCL Health Data Science MSc programme covers computational and statistical methods applied to large and complex biomedical data in order to improve health and healthcare through medical research. As part of this programme, you will gain an understanding of techniques that are transforming medical research and creating exciting new commercial opportunities.\nPrevious students have joined the lab group to undertake dissertation projects or as visiting members.\nMSc dissertation projects offered by the lab group:\n* 2022: Machine learning to quantify the impact of the COVID-19 pandemic on ICU admissions: an interrupted time-series analysis. (Supervisor: Chris Tomlinson)\n* 2022: Supervised machine learning for outcome prediction in routinely-collected Welsh ICU data. (Supervisor: Chris Tomlinson)\n* 2022: Unsupervised machine learning for clustering patient subgroups in routinely-collected Welsh ICU data. (Supervisor: Chris Tomlinson)"
  },
  {
    "objectID": "resources/homegrown/data-clinic-for-general.html",
    "href": "resources/homegrown/data-clinic-for-general.html",
    "title": "Data Clinic: A ‘How to’ Template",
    "section": "",
    "text": "Overview\nThis document aims to provide an overview of the Data Clinic methodology, to provide interested parties with a template for setting up their own Data Clinic.\nThe information contained here is purposefully brief, as the methodology will need to be adapted to your local environment. However, the general structure aims to be flexible and thus transferable between different environments.\n\n\nWhat is the ‘Data Clinic’?\nThe Data Clinic is a Quality Improvement initiative to help frontline healthcare workers in Critical Care get useful, relevant data from Electronic Health Record Systems.\nThis is to improve the ease and speed of audits, QIPs, service evaluations and other projects performed in Critical Care.\nPlease note: although we serve limited data for research feasibility projects, we do not serve data for full research projects. This is because this requires further information governance and ethical approvals. We only serve projects that fall under the information governance approvals for use of data for departmental audit and governance.\n\n\nWhat are the issues that the Data Clinic is trying to solve?\nWe know that data-driven organisation are more effective, whether in business or healthcare.\nTo this end, in 2019 UCLH invested millions of pounds to obtain an fully electronic health record system (EHRS) in the form of EPIC with the aim of becoming a leader in the field of technologically-enabled healthcare. Other trusts around the UK are following suit, with increasing adoption of EHRS.\nPrior to the Data Clinic, the method for obtaining data from EPIC was through contacting the Business Intelligence (BI) team. This is a team of Data analysts employed by the Trust whose role it is to monitor performance data in their department and service data requests. It is likely that trusts who use EHRS will have similar teams.\nThis process had a number of issues:\n\nThere was little day-to-day contact between clinical staff and the BI team. As a result, many clinical staff are unaware of who the BI team are, what they do & how to contact them.\n\nWhen clinical staff did manage to get in contact with the BI team, the process of requesting data usually took the form of numerous back-and-forth emails between clinical staff and the BI team: clarifying what data was required vs what is possible to extract from the EHRS. This process was often time-consuming and frustrating for both the clinical staff and BI team for the following reasons:\n&gt; - Clinical staff often have a poor understanding of the nuances of data from EHRS including what data is available and how it will actually be useful for the project’s aims. They often had unrealistic expectations of what data could be provided and in what time frame.\n&gt; - Although the BI team have a good understanding of how to find data in EPIC and manage it, they may lack the clinical understanding and direction to extract the correct data and to provide learning from it.\n\n\n\nHow does the Data Clinic address these issues?\nThe Data Clinic aims to improve the process of providing data to clinical staff by combining the knowledge of EHRS data and data analytic skills of the BI team with the clinical knowledge and acumen of a “Data Fellow”. This “Data Fellow” is a critical care practitioner (e.g. Doctor, nurse) who has knowledge of the day-to-day working of the Critical Care unit (and usually has an interest in data science/Audit/QI) and can act as a “translator” between the clinical staff and BI team. This core Data Clinic team is supervised by a senior figure in the department - usually a consultant with an interest if data/Audit/QI - who is able to provide direction for the service and help manage contentious issues.\nWe make the process of contacting this team apparent. Once contacted, the clinical staff requiring data for a project have the opportunity to meet with the team face-to-face to discuss their project and its data requirements.\nThrough this combination, the Data Clinic aims:\n\nTo better understand the clinical question the Requestor is trying to answer.\n\nUsing our knowledge of the data that is available, clarify exactly what data is required to achieve the project’s aims.\n\nProvide that data for the Requestor as quickly as possible, in the form that the Requestor can use.\n\nWe hypothesise that this benefits both clinical staff requesting the data and the BI team.\nClinical staff:\n\nMakes the process of getting data from EHRS clearer\n\nEnhances understanding of the data that is available from EHRS & how it will be useful for their project aims\n\nEnsures the data provided is in a format that is useful to the Requestor and sensitive to their data analytic skills (e.g. there is no point providing a 10,000 row spreadsheet to someone who struggles to use Excel)\n\nOverall makes it easier to get the data needed to complete audit/QIP/service evaluations (thereby increasing the number of projects completed, departmental performance and eventually improving patient care)\n\nBI team:\n\nEnsures the data requests being recieved are realistic and relevant to achieving the aims of projects\n\nThrough the Data Fellow, provides a “go-to” individual for questions about clinical queries\n\nBetter understanding of clinical relevance of their work\n\nSaves time/frustration emailing back-and-forth with clinical staff\n\n\n\nHow do you measure the success of the Data Clinic?\nThere are multiple metrics that can be used to define “success”. What you choose may reflect your department’s goals. Examples include: speed of delivery of data, number of projects completed, complexity or projects completed, etc.\nOur two main recommendations from our experience are:\n\nUse a mixed methods approach, collecting both quantitative and qualitative data.\n\nThe full impact of the service is unlikely to be captured by a single metric - or by just qualitative vs quantitative feedback. Using a mixed methods approach has allowed us to capture a more complete picture of the Data Clinic’s impact.\n\n\nCollect Outcome, Process and Balancing measures. Similarly to the above, it shall allow you gather a more complete picture of the Data Clinic’s impact.\n\nOutcome measures capture the Data Clinic’s impact on your main aims of the project (i.e. How do you define success?)\n\nProcess measures capture the impact of the processes that you are using to try to achieve your goals (i.e. How are you achieving our success?)\n\nBalancing measures capture any unintended consequences of your project, which may offset the benefits it is bringing (e.g. You may be delivering data quickly, but is it of sufficient quality? Are the BI team overworked?)\n\n\nListed below is our strategy for measuring the impact of the Data Clinic & the reasons behind this approach. This can be copied directly, or adapted to your local environment. We chose 6 key measurements to focus on collecting to avoid overburdening the team. In reality, it may be possible to collect more than this.\nOutcome measures:\n\nRequestor satisfaction with the data delivered\n\nRequestor satisfaction with the Data Clinic process\n\nProcess measures:\n\nTime taken to deliver the data\n\nComplexity of the data delivered (e.g. in terms of size of data provided, variety of data, detail of data)\n\nQualitative feedback on the Data Clinic experience\n\nBalancing measures:\n\nFeedback from the Data Clinic team on their experience\n\nWe chose Requestor satisfaction with the data delivered and the Data Clinic process as our main outcome metrics. We chose this as we felt that this captured the benefit of the data requesting process more holistically and was adaptable to the different types of projects served.\nWe reasoned that measures such as “Time taken to deliver data” or “Complexity of data delivered” were better suited as Process measures as they are mediators - but not guarantees - of overall satisfaction. In general, one would expect satisfaction to go up as data is delivered faster and with more detail. However, if data is delivered fast at the expense of quality or is not sensitive to the Requestors needs, then satisfaction may not high.\nWe also collected qualitative feedback from the Requestors and the Data Clinic team on the Data Clinic experience. This allowed us to identify particular areas of success/areas to be improved from both the Requestor and Data Clinic team viewpoints.\n\n\n(Current) Structure of the Data Clinic\nBelow is a brief, introductory overview of the current structure of the Data Clinic. As we run this as a Quality Improvement initiative, we are continually updating our structure/processes in response to feedback.\n\nData Clinic core team\nData Fellow: critical care practitioner (e.g. Doctor, nurse) who has knowledge of the day-to-day working of the Critical Care unit.\n- So far this has been an SHO-level doctor with some data science acumen who has a 1/3rd to 2/3rd split job role between Data Clinic and clinical work in ICU. However this could reasonably done by any medical professional with time/interest.\nData Analyst: a data scientist (or similar) who has experince and knowledge of the EHRS system and how to extract data from it. Clinical knowledge is not necessary as that should be provided by the Data Fellow.\nProject supervisor: a senior departmental figure (e.g. consultant) who is able to provide guidance/direction to the project, but is perhaps not involved in the day t day running. Benefits from some knowledge of data/Audit/QI.\n\n\nHow are requests for data recieved?\nHow requests for data are received will depend what works best for your local situation.\nWe currently accept requests using a dedicated Data Clinic email address which Requestors can send requests to. This email address is also used to communicate with Requestors throughout the project. We have found this to be a useful single point of contact for Requestors.\n\n\nProcess once a request is recieved\nOnce a request has been received, it shall be screened by a member of the Data Clinic team. It will then be allocated to follow one of two pathways:\n\nThe request can be served by an existing report or we have a data extraction script already prepared that serves the purpose\n\nExample 1: A Requestor wants a list of all patients discharged from ICU in last month. This can be be served by an existing discharge from ICU report on EPIC.\n\nExample 2: A Requestor is performing the second cycle of a QIP and would like to re-collect data to assess the impact of their change. If the Data Clinic performed the 1st cycle data extraction, we may be able to reuse the existing data extraction script (with updated dates) to re-collect the data they need post-intervention.\n\n\nIn this case, the data request can often be quickly and easily served by the Data analyst without the need for a meeting.\n\nThe request cannot be served by an existing report/data extraction script\n\nExample 1: A Requestor is conducting a new QIP and would like data on blood gas results performed throughout a patient’s ICU stay. This is not covered by existing reports (e.g. ICNARC report) and as it is a new project, we do not have an existing script to perform the data extraction.\n\n\nIn this case, it will be necessary for the Requestor to meet with the Data Clinic team to discuss their project and its data requirements.\nIf a request is allocated to pathway “2”, then a meeting is arranged between the Requestor and the Data Clinic team.\nAt this meeting, we discuss:\n- The project aims\n- The data requirements including inclusion/exclusion criteria for patients\n- How the data will help answer the project aims\n- What data is actually available on EHRS and how this may affect the project aims/scope\n- What format the Requestor requires the data to be in (e.g. do they need an excel spreadsheet with raw data, or do they need the data processed into summary tables/graphs/etc)\nFollowing this meeting, the Data Clinic team will prepare the data extraction (or a limited subset of the full extraction).\nOnce this is done, the Data Clinic team meet again with the Requestor to show them the data extraction.\n- If the Requestor is happy with this data, the Data Clinic team then proceed to a full data extraction and deliver it to the Requestor.\n- If the Requestor are not satisfied, the extraction is revised according to the feedback received. Further meetings are then arranged with the Requestor after each round of extraction, until the data provided is adequate.\n(see the below flowchart for a graphical representation of this process)\n\n\n\n\n\nflowchart TD\nA(Data request received) --&gt; B(Screened by Data Clinic team)\nB --&gt; C(Meeting with Data Clinic team)\nB --&gt; K(Can be served by existing report)\nC --&gt; D[1st extraction of data]\nD --&gt; F{2nd meeting between requestor & Data Clinic team}\nF --&gt; G[Happy with data]\nF --&gt; H[Revisions needed]\nG --&gt; I(Full extraction & data delivered)\nH --&gt; J(Further revision to extraction)\nJ --&gt; F\nK --&gt; L(Data delivered)\n\nlinkStyle default stroke: black;\n\n\n\n\n\n\n\n\n\nMeasurement techniques\nTo collect the data on the Outcome/Process/Balancing measures above, we used the following techniques:\n1) Feedback surveys sent to Requestors\n\n2 surveys were used:\n1. “Immediate” feedback survey. This was sent to Requestors immediately after the request had been served. This focused on their experience of coming through the Data Clinic - and if they were satisfied with the process.\n2. “Delayed” feedback survey. This was sent to Requestors weeks-to-months later, after the Requestors have had time to use the data for their project. This focused on their satisfaction with the data delivered. By this point they will have had the opportunity to use the data for the purposes of their project, so can make a judgement on its true usefulness.\nThese feedback surveys also contain questions on other metrics our team are interested in (e.g. what barriers to using data did the Data Clinic help overcome)\n\n2) After-Action Review meetings between Data Clinic team members\n\nThese meetings are performed approximately every 3 months. We sit together and review the completed projects during that time.\nWe start by reviewing the quantitative metrics for the projects (e.g. how long did it take).\nWe then collect the thoughts of the Data Clinic team members on the project and how it went. We use an formal After-Action Review template to structure this discussion and identify areas for improvement.\nAs well as identifing areas that went well, these meetings provide a forum for Data Clinic team members to raise any issues they have identified (i.e. Balancing measures).\n\nTemplates for the feedback surveys and the After-Action Review meetings can be accessed by emailing one of the UCLH Data Clinic team members (see below).\n\n\nWords of wisdom & lessons learnt\nSince its inception, the Data Clinic has been a process of constant learning and development. We have shared below some of the lessons we have learnt, which may be relevant to your implementation.\n1. Help with project methodology\nThe process of coming through Data Clinic often reveals that projects are not are thoroughly considered as they first appear (and this applies for all grades/seniority of Requestors!) or that the realities of the data available leads to significant reformulation of the project aims/methodology.\nOne of the strengths of the Data Clinic has been the opportunity for the Requestor to meet face-to-face with the Data Clinic team to discuss their project before starting the data extraction. This meeting often reveals issues with the project’s inclusion criteria or methodology - something the Data Fellow is often able to spot using their clinical experience. Often these issues can be resolved in this first meeting which makes the subsequent data extraction more straightforward for the data analyst.\nHowever, there are occasions where it is obvious the project needs further consideration before progressing. It has therefore been enormous helpful for us to partner with Audit/QI/Methodology leads and experts within our department. We can now “refer” projects to these individuals if they need extra methodology support. Once they have been through this process, they come back to us with much clearer aims and considered data requests.\n2. Types of requests accepted\nWe initially limited the scope of the Data Clinic to just serving requests for Audit or QI projects.\nThis proved useful for limiting our workload while we developed our service. However, it quickly became apparent that many requests for projects that had clear potential benefits for the department did not fall into the category of Audit/QI.\nAs a result, we expended the scope of the projects we serve to:\n- Audits\n- QI\n- Service evaluations\n- Data for business cases\n- Research feasibility projects\n- Help with simple data analysis for existing datasets\n- Others (at the discretion of whether the Data Clinic team feels it is reasonable)\nThis has increased the numbers of projects we serve and has certainly increased the usefulness of the Data Clinic to the department.\nHowever, it does have the downside of increasing workload, so perhaps caution is warranted when first starting.\n3. Inbalances between the requested data and the data that is useful\nOne of the strengths of electronic health records is the ability to easily collect large volumes of data. However, we have found this can present unexpected challenges for clinical staff completing projects.\nRequestors can often ask for more data than they need or can handle. This can be requesting superfluous variables which are not relevant to the project’s aims or analyses. Alternatively, the volume of data produced can be overwhelming for Requestors who are used to smaller, manually collected datasets - especially for those with limited data science skills.\nIt is therefore important to establish early on what variables are truly important for the project and to try to limit the extraction to these. It is usually straightforward to add variables to existing requests if it later becomes apparent that extra data is needed. This ensures that the datasets produced are relevant to their purpose - limiting information overload for the Requestors and misuse of the valuable data analyst’s time in extracting unnecessary data.\n\n\nContact details for the UCLH Data Clinic\nIf you have questions on the above, or would like to discuss further, please email one of the individuals below (Dr Campbell will be the 1st point of call for most items).\nDr Ruaraidh Campbell: Lead Data Fellow for Data Clinic (ruaraidh.campbell@nhs.net)\nHumayra Chowdhury: BI analyst (humayra.chowdhury@nhs.net)\nDr Tim Bonnici: Data Lead for Critical Care (t.bonnici@nhs.net)\n\n\nAcknowledgements\nThe Data Clinic has only been possible due to the contribution of numerous indivduals in addition to those listed above. We would like to acknowledge the time and effort that have put into this project.\nData Fellows (Past and Present): Dr Conor Foley [Lead Fellow for Data Clinic 2021/2022], Dr Dan Stein, Dr Hrisheekesh Vaidya, Dr Sarah Vaughan\nBusiness Intelligence Partners (Past and Present): Tracey Crissell, David Egan"
  },
  {
    "objectID": "resources/homegrown/cleanehr.html",
    "href": "resources/homegrown/cleanehr.html",
    "title": "cleanEHR",
    "section": "",
    "text": "LEGACY: No longer maintained\nThe security put in place to ensure the safety of this resource inevitably creates challenges for the researchers. We have therefore created a three sided tool to make CCHIC research ready.\n\nthis shared code library\nan anonymised development data set\na virtual machine for simulating work within the safe haven\n\nYou request access to the anonymised toy dataset from here"
  },
  {
    "objectID": "resources/awesome.html",
    "href": "resources/awesome.html",
    "title": "Recommendations",
    "section": "",
    "text": "A curated list of awesome resources recommended by our lab group.\n\n\n    \n      \n      \n    \n\n\n\n  \n    Causal Inference: What If?\n    \n      Steve Harris  | Nov 22, 2022\n      \n       I read drafts of this book during my Phd, and loved it. Miguel Hernan writes beautifully, and I found his reasoning and explanations very intuitive. \n    \n  \n\n  \n    Code with Engineering Playbook\n    \n      Steve Harris  | Dec 4, 2022\n      \n       Recommendations from Microsoft's Consumer Software Engineering. This is full of awesomeness. \n    \n  \n\n  \n    DAGitty — draw and analyze causal diagrams\n    \n      Steve Harris  | Nov 25, 2022\n      \n       DAGitty is a browser-based environment for creating, editing, and analyzing causal diagrams (also known as directed acyclic graphs or causal Bayesian networks). The focus is on the use of causal diagrams for minimizing bias in empirical studies in epidemiology and other disciplines. \n    \n  \n\n  \n    HealthyR: R for Health Data Science\n    \n      Ruaraidh Campbell  | Nov 19, 2022\n      \n       A great free beginner introduction to R for healthcare professionals!  Created by Prof Ewan Harrison (a professor of Surgical & Data Science at Edinburgh University), this course takes you from an absolute beginner, with topics such as \"what is R?\" & \"how to install it\", through to running your own regression analyses and producing plots!\n \n    \n  \n\n  \n    JAMA Guide to Statistics and Methods\n    \n      Steve Harris  | Nov 22, 2022\n      \n       I found this through this article [Why Test for Proportional Hazards?](https://doi.org/10.1001/jama.2020.1267) by the amazing Miguel Hernan (proposer of 'Causal Inference: What If')\n \n    \n  \n\n  \n    Pytorch Lightning\n    \n      Ahmed Al-Hindawi  | Dec 14, 2022\n      \n       An opinionated framework for developing models on-top of PyTorch. It handles moving tensors to the appropriate device, batching, and multi-GPU/TPU training and inference. Also comes with a set of metrics suitable for the assessment of the model out of the box \n    \n  \n\n  \n    ROS\n    \n      Ahmed Al-Hindawi  | Dec 14, 2022\n      \n       The Robot Operating System (ROS) is a framework that has gained widespread adoption for asynchronous publisher-subscriber model aiming at making all things Robots easier. \n    \n  \n\n  \n    The Art of the Command Line\n    \n      Steve Harris  | Nov 25, 2022\n      \n       This is a selection of notes and tips on using the command-line that we've found useful when working on Linux. Some tips are elementary, and some are fairly specific, sophisticated, or obscure. This page is not long, but if you can use and recall all the items here, you know a lot. \n    \n  \n\n  \n    daiquiri\n    \n      Steve Harris  | Dec 5, 2022\n      \n       The daiquiri package generates data quality reports that enable quick visual review of temporal shifts in record-level data.\n \n    \n  \n\n  \n    einops\n    \n      Ahmed Al-Hindawi  | Dec 14, 2022\n      \n       A framework agnostic library that makes the task of reshaping, reducing, and repeating tensor shapes easy. \n    \n  \n\n  \n    timm\n    \n      Ahmed Al-Hindawi  | Dec 14, 2022\n      \n       Torch Image Models (TIMM) s a collection of computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations and also training/validating scripts with ability to reproduce ImageNet training results \n    \n  \n\n\n\nNo matching items",
    "crumbs": [
      "Resources",
      "Recommendations"
    ]
  },
  {
    "objectID": "tutorials/hylode/hylode.html",
    "href": "tutorials/hylode/hylode.html",
    "title": "HYLODE documentation",
    "section": "",
    "text": "HYLODE documentation\nDocumentation specific to the HYLODE project",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HYLODE documentation"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_1_training.html",
    "href": "tutorials/hylode/vignette_1_training.html",
    "title": "Training data",
    "section": "",
    "text": "HyCastle, the lens and building a training set\nIn the previous notebook, we showed an end-to-end exemplar of the Hylode platform - but we skated top speed over some details worth spending more time on.\nHere we take a more measured pace and zoom in on HyCastle and the lens. Together these two abstractions make it easy to ask Hylode for a retrospective training set and then to pre-process that training set.\nHylode does this in a way that allows the same underlying code to furnish the live data needed for deployable prediction.",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Training data"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_1_training.html#hycastle",
    "href": "tutorials/hylode/vignette_1_training.html#hycastle",
    "title": "Training data",
    "section": "HyCastle",
    "text": "HyCastle\nThe HyCastle module is the main workhorse for pulling the complete available feature set out of hylode_db (Hylode’s internal databases). Having defined our features in HyGear (covered in vignette 3), HyCastle can do two main things:\n~ it can pick out a training set comprising all the features for each hourly slice for each patient\n~ it can give us a live set of features for the patients currently on the ward\nLet’s try it out…\n\nfrom hycastle.icu_store.retro import retro_dataset\nfrom hycastle.icu_store.live import live_dataset # &lt;-- includes PII\n\nward = 'T03'\n\n\n# the retro_dataset function gives us all the historical episode slices to build up our training set\ntrain_df = retro_dataset(ward)\ntrain_df.shape\n\n\n# and we can see the various feature columns we have generated\ntrain_df.head()\n\nThen using the same machinery, we can get the corresponding features for the patients currently on the ward.\nWhy this is important is that the same code is generating our training features and the features we will use to deploy the model (- ruling out unwanted surprises from divergence between the two!)\n\npredict_df = live_dataset(ward)\npredict_df.shape\n\n\npredict_df['horizon_dt'].head()",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Training data"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_1_training.html#the-lens",
    "href": "tutorials/hylode/vignette_1_training.html#the-lens",
    "title": "Training data",
    "section": "The lens",
    "text": "The lens\nIn the code above, we saw that HyCastle is very nifty in delivering us all the features we have pre-defined in hylode_db. But the question naturally arises, what if we want to use a subset of those features? Or to pre-process them in a specific way?\nWill this not require custom code - exposing us to the same risk of code divergence between training and deployment?\nOur answer to this is the lens. It is an abstraction that provides a more robust (transferrable) way to subset and pre-process the features coming out of HyCastle. Let’s have a look at a very simple example.\n\nfrom hycastle.lens.base import BaseLens\nfrom typing import List\nfrom sklearn.compose import ColumnTransformer\nfrom hycastle.lens.transformers import DateTimeExploder\n\n\nclass SimpleLens(BaseLens):\n    numeric_output = True\n    index_col = \"episode_slice_id\"\n\n    @property\n    def input_cols(self) -&gt; List[str]:\n        return [\n            \"episode_slice_id\",\n            \"admission_dt\",\n        ]\n\n    def specify(self) -&gt; ColumnTransformer:\n        return ColumnTransformer(\n            [\n                (\n                    \"select\",\n                    \"passthrough\",\n                    [\n                        \"episode_slice_id\"\n                    ],\n                ),\n                (\n                    \"admission_dt_exp\",\n                    DateTimeExploder(),\n                    [\"admission_dt\"],\n                ),\n            ]\n        )\n\nNotice that what we really have here is a list of 3-tuples to initialise the ColumnTransformer (which is a standard SKLearn class). For instance, the triple:\n                (\n                    \"admission_dt_exp\",\n                    DateTimeExploder(),\n                    [\"admission_dt\"],\n                )\nLet’s see what happens when we put this lens to work on the output from HyCastle\n\nlens = SimpleLens()\n\nX = lens.fit_transform(train_df)\nX.head()\n\n…basically we seem to have the episode_slice_id for every slice, and then a bunch of features about the admission_dt. In our original HyCastle dataset, we notice that admission_dt is a series of datetimes:\n\ntrain_df['admission_dt'].head()\n\n…but after we have transformed the retro dataframe, we have these additional admission features. This is thanks to the triple quoted above and the DateTimeExploder(). Let’s have a look to see what that code looks like…\n\n??DateTimeExploder\n\n\n??DateTimeExploder.transform\n\nIn short, what we are doing in defining a lens is defining a set of input columns from HyCastle that we want to work with, and then a sequence of column transformations (as a ColumnTransformer object) that we use to specifically define our pre-processing pathway.\nThis lens can then be used consistently between model training and deployment.",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Training data"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_1_training.html#appendix-1-a-more-complete-example",
    "href": "tutorials/hylode/vignette_1_training.html#appendix-1-a-more-complete-example",
    "title": "Training data",
    "section": "Appendix 1: A more complete example",
    "text": "Appendix 1: A more complete example\nHere’s a fuller and more complete example of a lens (along the lines of what we will use in the next vignette).\nIt might be worthwhile using the ?? shortcut to get a sense of the different transformations being applied.\n\nfrom sklearn.preprocessing import (\n    FunctionTransformer,\n    OneHotEncoder,\n    OrdinalEncoder,\n    StandardScaler,\n)\nfrom sklearn.impute import MissingIndicator, SimpleImputer\nfrom hycastle.lens.transformers import timedelta_as_hours\n\n\nclass DemoLens(BaseLens):\n    numeric_output = True\n    index_col = \"episode_slice_id\"\n\n    @property\n    def input_cols(self) -&gt; List[str]:\n        return [\n            \"episode_slice_id\",\n            \"admission_age_years\",\n            \"avg_heart_rate_1_24h\",\n            \"max_temp_1_12h\",\n            \"avg_resp_rate_1_24h\",\n            \"elapsed_los_td\",\n            \"admission_dt\",\n            \"horizon_dt\",\n            \"n_inotropes_1_4h\",\n            \"wim_1\",\n            \"bay_type\",\n            \"sex\",\n            \"vent_type_1_4h\",\n        ]\n\n    def specify(self) -&gt; ColumnTransformer:\n        return ColumnTransformer(\n            [\n                (\n                    \"select\",\n                    \"passthrough\",\n                    [\n                        \"episode_slice_id\",\n                        \"admission_age_years\",\n                        \"n_inotropes_1_4h\",\n                        \"wim_1\",\n                    ],\n                ),\n                (\"bay_type_enc\", OneHotEncoder(), [\"bay_type\"]),\n                (\n                    \"sex_enc\",\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-1\n                    ),\n                    [\"sex\"],\n                ),\n                (\n                    \"admission_dt_exp\",\n                    DateTimeExploder(),\n                    [\"admission_dt\", \"horizon_dt\"],\n                ),\n                (\n                    \"vent_type_1_4h_enc\",\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-1\n                    ),\n                    [\"vent_type_1_4h\"],\n                ),\n                (\n                    \"vitals_impute\",\n                    SimpleImputer(strategy=\"mean\", add_indicator=False),\n                    [\n                        \"avg_heart_rate_1_24h\",\n                        \"max_temp_1_12h\",\n                        \"avg_resp_rate_1_24h\",\n                    ],\n                ),\n                (\n                    \"elapsed_los_td_hrs\",\n                    FunctionTransformer(timedelta_as_hours),\n                    [\"elapsed_los_td\"],\n                ),\n            ]\n        )\n\n\nlens = DemoLens()\n\nX = lens.fit_transform(train_df)\nX.head()\n\n\nX.dtypes",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Training data"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_0_intro.html",
    "href": "tutorials/hylode/vignette_0_intro.html",
    "title": "Exemplar",
    "section": "",
    "text": "Let’s begin an end-to-end modelling exemplar. The whole pipeline in c. 20 lines. In the previous section, we made some grand claims about Hylode addressing:\n~ train-deploy split\n~ feature provision\n~ time-series modelling\n~ model management\n~ transition to deployment\nLet’s see a concrete example of how this looks.\n\nRetrospective training set. Extraction & pre-processing\nFor the sake of these notebooks, we’re going to consider the problem of modelling ICU discharge at 48 hours.\n\n# First off, Hylode immediately gives us the features we need to train a time series model\nfrom hycastle.icu_store.retro import retro_dataset\n\ntrain_df = retro_dataset('T03')\ntrain_df.shape\n\nThis single piece of code has done a lot of work behind the scenes.\nIt has pulled data from EMAP, processed it where necessary to create features and then cut those features up so we have one row per hour for each patient – setting us up to make live predictions every hour for each patient on the unit.\nLet’s have a look at which features we have:\n\ntrain_df.columns\n\nThis is great, but we still have some categorical variables etc. laced in there. What happens if I want to do some pre-processing?\nThe answer lies in our lens abstraction, let’s have a look at one I prepared earlier:\n\nfrom hycastle.lens.icu import BournvilleICUSitRepLens\nlens = BournvilleICUSitRepLens()\n\nX_train = lens.fit_transform(train_df)\nX_train.columns\n\nWe can see we’ve just done some useful things. The lens’s fit_transform method has inserted missingness values for ethnicity, and we have broken out each patient’s admissions time into separate features as we think that will improve our model.\nWe also define a label:\n\ny_train = train_df['discharged_in_48hr'].astype(int)\n\n\n\nTraining & storing a model\nWith this “heavy” lifting done, we should now already be in a position to train a model. Let’s have a go:\n\nfrom sklearn.ensemble import RandomForestClassifier\nm = RandomForestClassifier(n_jobs=-1)\nm.fit(X_train.values, y_train.values.ravel())\n\nEverything seems to be working. Let’s imagine we’re happy with what we’ve done. Let’s log the model in our model repo, so it’s primed and ready to deploy…\n\nimport os\nimport mlflow\nmlflow_server = os.getenv('HYMIND_REPO_TRACKING_URI')\nmlflow.set_tracking_uri(mlflow_server)\n\n\nfrom datetime import datetime\nimport random\nimport string\n\n# Generate a unique experiment name\nexp_name = \"vignette_0-\" + \"\".join( random.sample(string.ascii_lowercase, k=8)) + str(datetime.now().timestamp())\n\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"] = exp_name\nexperiment_id = mlflow.create_experiment(exp_name)\n\nexperiment_id\n\n\nwith mlflow.start_run():\n    mlflow.sklearn.log_model(m, 'model')\n\nThis screenshot shows what it looks like for the model to land safely in MLFlow (which you should be able to see for yourself if you follow the link here – look for a new experiment at the very bottom of the list on the left hand side)\n\n\nLoading and deploying a model\nNow let’s switch hats and imagine they were are trying to deploy the model in silent mode for the patients currently on the ICU. This is now pretty straightforward:\n\nfrom hycastle.icu_store.live import live_dataset\nlive_df = live_dataset('T03')\nlive_df.shape\n\n\nlive_df.columns\n\n\nX_df = lens.transform(live_df)\n\n\nruns = mlflow.search_runs(experiment_ids=[experiment_id])\nrun_id = runs.iloc[0].run_id\nrun_id\n\n\nlogged_model = f'runs:/{run_id}/model'\nloaded_model = mlflow.sklearn.load_model(logged_model)\n\n\npredictions = loaded_model.predict_proba(X_df.values)\nlive_df['prediction'] = predictions[:, 1]\nlive_df.loc[:, ['bed_code', 'prediction']].head()",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Exemplar"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-ML-example.html",
    "href": "tutorials/hylode/HyMind-ML-example.html",
    "title": "HyMind Machine Learning",
    "section": "",
    "text": "from typing import List\nimport os\nimport tempfile\nfrom pathlib import Path\nimport pickle\nfrom uuid import uuid4\n\nimport cloudpickle\nimport pandas as pd\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import ParameterGrid, train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import MissingIndicator, SimpleImputer\nfrom sklearn.pipeline import Pipeline as SKPipeline\nfrom sklearn.preprocessing import (\n    FunctionTransformer,\n    OneHotEncoder,\n    OrdinalEncoder,\n    StandardScaler,\n)\n\nfrom hycastle.lens.base import BaseLens\nfrom hycastle.lens.transformers import DateTimeExploder, timedelta_as_hours\n\n%matplotlib inline\n\n\nsecret_path = 'secret'\nos.environ['EMAP_DB_USER'], os.environ['EMAP_DB_PASSWORD'] = Path(secret_path).read_text().strip().split('\\n')\n\n\nfrom hylib.dt import LONDON_TZ\nfrom hycastle.lens.icu import BournvilleICUSitRepLens\nfrom hycastle.icu_store.live import live_dataset\nfrom hycastle.icu_store.retro import retro_dataset\nfrom hymind.lib.models.icu_aggregate import AggregateDemandModel",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Machine Learning"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-ML-example.html#imports",
    "href": "tutorials/hylode/HyMind-ML-example.html#imports",
    "title": "HyMind Machine Learning",
    "section": "",
    "text": "from typing import List\nimport os\nimport tempfile\nfrom pathlib import Path\nimport pickle\nfrom uuid import uuid4\n\nimport cloudpickle\nimport pandas as pd\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import ParameterGrid, train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.compose import ColumnTransformer, make_column_selector\nfrom sklearn.impute import MissingIndicator, SimpleImputer\nfrom sklearn.pipeline import Pipeline as SKPipeline\nfrom sklearn.preprocessing import (\n    FunctionTransformer,\n    OneHotEncoder,\n    OrdinalEncoder,\n    StandardScaler,\n)\n\nfrom hycastle.lens.base import BaseLens\nfrom hycastle.lens.transformers import DateTimeExploder, timedelta_as_hours\n\n%matplotlib inline\n\n\nsecret_path = 'secret'\nos.environ['EMAP_DB_USER'], os.environ['EMAP_DB_PASSWORD'] = Path(secret_path).read_text().strip().split('\\n')\n\n\nfrom hylib.dt import LONDON_TZ\nfrom hycastle.lens.icu import BournvilleICUSitRepLens\nfrom hycastle.icu_store.live import live_dataset\nfrom hycastle.icu_store.retro import retro_dataset\nfrom hymind.lib.models.icu_aggregate import AggregateDemandModel",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Machine Learning"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-ML-example.html#mlflow-init",
    "href": "tutorials/hylode/HyMind-ML-example.html#mlflow-init",
    "title": "HyMind Machine Learning",
    "section": "MLFlow Init",
    "text": "MLFlow Init\n\nmlflow_var = os.getenv('HYMIND_REPO_TRACKING_URI')\nmlflow.set_tracking_uri(mlflow_var)   \n\n\nclient = MlflowClient()",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Machine Learning"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-ML-example.html#data",
    "href": "tutorials/hylode/HyMind-ML-example.html#data",
    "title": "HyMind Machine Learning",
    "section": "Data",
    "text": "Data\n\ndf = retro_dataset('T03')\n\n\ndf.shape\n\n\ndf.head()\n\n\nLens\n\n# lens = BournvilleICUSitRepLens()\n\n\nclass DemoLens(BaseLens):\n    numeric_output = True\n    index_col = \"episode_slice_id\"\n\n    @property\n    def input_cols(self) -&gt; List[str]:\n        return [\n            \"episode_slice_id\",\n            \"admission_age_years\",\n            \"avg_heart_rate_1_24h\",\n            \"max_temp_1_12h\",\n            \"avg_resp_rate_1_24h\",\n            \"elapsed_los_td\",\n            \"admission_dt\",\n            \"horizon_dt\",\n            \"n_inotropes_1_4h\",\n            \"wim_1\",\n            \"bay_type\",\n            \"sex\",\n            \"vent_type_1_4h\",\n        ]\n\n    def specify(self) -&gt; ColumnTransformer:\n        return ColumnTransformer(\n            [\n                (\n                    \"select\",\n                    \"passthrough\",\n                    [\n                        \"episode_slice_id\",\n                        \"admission_age_years\",\n                        \"n_inotropes_1_4h\",\n                        \"wim_1\",\n                    ],\n                ),\n                (\"bay_type_enc\", OneHotEncoder(), [\"bay_type\"]),\n                (\n                    \"sex_enc\",\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-1\n                    ),\n                    [\"sex\"],\n                ),\n                (\n                    \"admission_dt_exp\",\n                    DateTimeExploder(),\n                    [\"admission_dt\", \"horizon_dt\"],\n                ),\n                (\n                    \"vent_type_1_4h_enc\",\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-1\n                    ),\n                    [\"vent_type_1_4h\"],\n                ),\n                (\n                    \"vitals_impute\",\n                    SimpleImputer(strategy=\"mean\", add_indicator=False),\n                    [\n                        \"avg_heart_rate_1_24h\",\n                        \"max_temp_1_12h\",\n                        \"avg_resp_rate_1_24h\",\n                    ],\n                ),\n                (\n                    \"elapsed_los_td_hrs\",\n                    FunctionTransformer(timedelta_as_hours),\n                    [\"elapsed_los_td\"],\n                ),\n            ]\n        )\n\n\nlens = DemoLens()\n\n\nX = lens.fit_transform(df)\n\n\ny = df['discharged_in_48hr'].astype(int)\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n\n\n\nDummy run\n\nm = RandomForestClassifier(n_jobs=-1, n_estimators=50, max_depth=2)\n%time m.fit(X_train.values, y_train.values.ravel())",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Machine Learning"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-ML-example.html#experiment",
    "href": "tutorials/hylode/HyMind-ML-example.html#experiment",
    "title": "HyMind Machine Learning",
    "section": "Experiment",
    "text": "Experiment\n\nUtils\n\ntmp_path = Path('tmp')\ntmp_path.mkdir(parents=True, exist_ok=True)\n\ndef mlflow_log_string(text, filename):\n    full_path = tmp_path / filename\n    with open(full_path, 'w') as f:\n        f.write(str(text))\n    mlflow.log_artifact(full_path)\n\ndef mlflow_log_tag_dict(tag_dict, filename):\n    \"\"\"Logs tag dict to MLflow (while preserving order unlike mlflow.log_dict)\"\"\"\n    full_path = tmp_path / filename\n    with open(full_path, 'w') as f:\n        yaml.dump(tag_dict, f, sort_keys=False)\n    mlflow.log_artifact(full_path)\n    \ndef mlflow_log_lens(l):\n    full_path = l.pickle(tmp_path)\n    mlflow.log_artifact(full_path, 'lens')\n\n\n# Owner|Type|Name|Date\nexp_name = 'NS|models|jendemo|2021-10-05'\n\n\nos.environ[\"MLFLOW_EXPERIMENT_NAME\"] = exp_name\nexperiment_id = mlflow.create_experiment(exp_name)\n\nexperiment_id\n\n\ndef artifact_path():\n    pth = Path(mlflow.get_artifact_uri())\n    pth.mkdir(parents=True, exist_ok=True)\n    return pth\n\n\n\nParameter Grid\n\ngrid = {\n    'n_estimators':[5, 10],\n    'max_depth':[2, 10]\n}\n\n\n\nRun\n\nruns_per_param_set = 2\n\nfor i in range(runs_per_param_set):\n    \n    for g in ParameterGrid(grid):\n        m = RandomForestClassifier(n_jobs=-1)\n\n        with mlflow.start_run():\n            #mlflow_logs()\n            \n            m.set_params(**g)\n            mlflow.log_params(g)\n\n            m.fit(X_train.values, y_train.values.ravel())\n            \n            eval_df = pd.DataFrame({\n                        'predict_proba':m.predict_proba(X_valid.values)[:,1], \n                        'label':y_valid.to_numpy().ravel()\n                       }, \n                columns=['predict_proba','label'])\n            \n            train_accuracy = m.score(X_train, y_train.to_numpy())\n            mlflow.log_metric('train_accuracy', train_accuracy)\n            valid_accuracy = m.score(X_valid, y_valid.to_numpy())       \n            mlflow.log_metric('valid_accuracy', valid_accuracy)\n            \n            train_confusion = confusion_matrix(m.predict(X_train.values), y_train.to_numpy())\n            mlflow_log_string(train_confusion, 'train_confusion.txt')\n            valid_confusion = confusion_matrix(m.predict(X_valid.values), y_valid.to_numpy())\n            mlflow_log_string(valid_confusion, 'valid_confusion.txt')\n\n            mlflow.sklearn.log_model(m, 'model')\n\n\n\nSelect Best Run\n\nruns = mlflow.search_runs()\nruns.head()\n\n\nparams = [col for col in runs if col.startswith('params')]\nbest_params = runs.groupby(params)['metrics.valid_accuracy'].mean().idxmax()\nbest_row = runs.set_index(keys=params).loc[best_params]\n\nbest_run_id = list(best_row['run_id'])[0]\nbest_run_id\n\n\nTag Best Run\n\nwith mlflow.start_run(run_id=best_run_id):\n    # tag the run as best_row\n    mlflow.set_tag('best_run', 1)   \n\n\n\nLog Lens\n\nwith mlflow.start_run(run_id=best_run_id):\n     mlflow_log_lens(lens)\n\n\n\n\nRegister Model from Best Run for Deployment\n\nmodel_name = 'demo-model-jen'\nversion = 1\n\n\nmlflow.register_model(f'runs:/{best_run_id}/model', model_name)",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Machine Learning"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-ML-example.html#simplified-inference-pathway",
    "href": "tutorials/hylode/HyMind-ML-example.html#simplified-inference-pathway",
    "title": "HyMind Machine Learning",
    "section": "Simplified Inference Pathway",
    "text": "Simplified Inference Pathway\n\nFind Registered Model\n\nmodel_info = client.get_model_version(model_name, version)\nmodel_info\n\n\nrun_info = client.get_run(model_info.run_id)\nrun_info\n\n\n\nLoad Model using Name & Version\n\nmodel = mlflow.sklearn.load_model(f'models:/{model_name}/{version}')\n\n\nmodel\n\n\nGet logged Lens\n\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp_dir = Path(tmp)\n    \n    client.download_artifacts(model_info.run_id, 'lens', tmp_dir)\n    \n    lens_path = next((tmp_dir / 'lens').rglob('*.pkl'))\n    with open(lens_path, 'rb') as f:\n        loaded_lens = pickle.load(f)\n\n\nloaded_lens",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Machine Learning"
    ]
  },
  {
    "objectID": "tutorials/hylode/HyMind-ML-example.html#predict-individual",
    "href": "tutorials/hylode/HyMind-ML-example.html#predict-individual",
    "title": "HyMind Machine Learning",
    "section": "Predict Individual",
    "text": "Predict Individual\n\nlive_df = live_dataset('T03')\n\n\nlive_df.loc[:, ['episode_slice_id', 'admission_dt', 'bed_code', 'avg_heart_rate_1_24h']].sort_values('admission_dt', ascending=False).head()\n\n\nX_df = loaded_lens.transform(live_df)\n\n\npredictions = model.predict_proba(X_df)\n\n\nlive_df['prediction'] = predictions[:, 1]\n\nlive_df.loc[:, [‘episode_slice_id’, ‘prediction’]]",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "HyMind Machine Learning"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_2_modelling.html",
    "href": "tutorials/hylode/vignette_2_modelling.html",
    "title": "Modelling",
    "section": "",
    "text": "In this notebook, we look at how the various different pieces of the Hylode architecture come together to ease the ML4H model development/deployment process.\nIn vignette_1_training_set, we looked at how HyCastle and the lens abstraction make for consistent training pathways between model development and deployment.\nHere, we bring these components together in a modelling workflow. Core steps are to show:",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Modelling"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_2_modelling.html#a-fuller-example",
    "href": "tutorials/hylode/vignette_2_modelling.html#a-fuller-example",
    "title": "Modelling",
    "section": "A fuller example",
    "text": "A fuller example\nWith slightly better sense of how MLFlow works, we now turn to a fuller exemplar workflow. The example we look at here is running a simple parameter grid search for a Random Forest model of ICU discharge at 48 hours.\n\n# the two most influential parameters \n# cf. https://scikit-learn.org/stable/modules/ensemble.html#parameters\ngrid = {\n    'n_estimators':[10, 50, 100],\n    'max_features':[None, \"sqrt\", \"log2\"]\n}\n\n\n# as the outcome of each training run (even with the same parameters) is non-deterministic,\n# we run two training runs per parameter combination.\nruns_per_param_set = 2\n\nfor i in range(runs_per_param_set):\n    \n    for g in ParameterGrid(grid):\n        m = RandomForestClassifier(n_jobs=-1)\n\n        with mlflow.start_run():\n            \n            # logging the tag dictionary, the run_type\n            mlflow_log_tag_dict(tag_dict, 'tag_dict.yaml')\n            mlflow.set_tag(\"run_type\", \"training\")\n            \n            # set and log this run's set of model parameters\n            m.set_params(**g)\n            mlflow.log_params(g)\n\n            m.fit(X_train.values, y_train.values.ravel())\n            \n            # calculate and log training and validation set accuracy\n            train_accuracy = m.score(X_train.values, y_train.to_numpy())\n            mlflow.log_metric('train_accuracy', train_accuracy)\n            valid_accuracy = m.score(X_valid.values, y_valid.to_numpy())       \n            mlflow.log_metric('valid_accuracy', valid_accuracy)\n            \n            # ditto for confusion matrices\n            train_confusion = confusion_matrix(m.predict(X_train.values), y_train.to_numpy())\n            mlflow_log_string(train_confusion, 'train_confusion.txt')\n            valid_confusion = confusion_matrix(m.predict(X_valid.values), y_valid.to_numpy())\n            mlflow_log_string(valid_confusion, 'valid_confusion.txt')\n\n            # store the trained SKLearn model, so we can check it out later\n            mlflow.sklearn.log_model(m, 'model')\n\nAfter this cells runs (which takes a minute or two), if you now return to the MLFlow UI, you will see that the experiment you created is now populated with a whole list of runs, one for each parameter set above. Clicking down into the run you will see all the attributes above have been stored (the tag dictionary, the parameters, the metrics, the model etc.)\nAs a next step, we might want to pick out the model parameters that seem to have performed best - so we can then use these for further evaluation.\nThese runs can also be straightforwardly access from a notebook using mlflow.search_runs()…\n\nruns = mlflow.search_runs()\nruns.head()\n\nFrom which starting point, it’s simple to mark out the parameter set with the best mean validation accuracy, as follows:\n\nparams = [col for col in runs if col.startswith('params')]\nbest_params = runs.groupby(params)['metrics.valid_accuracy'].mean().idxmax()\nbest_row = runs.set_index(keys=params).loc[best_params]\n\nbest_run_id = list(best_row['run_id'])[0]\nbest_run_id\n\nAnd then we can tag this as the best run from our training loop - and also log the lens we used to train it:\n\nwith mlflow.start_run(run_id=best_run_id):\n    # tag the run as best_row\n    mlflow.set_tag('best_run', 1)   \n\n    # log the lens\n    mlflow_log_lens(lens)\n\nIn the same breath, MLFlow gives us the option to register our model, which makes it easy to access and work with going forward - so let’s do that too:\n\n# =&gt; add a unique model name below &lt;=\n# e.g. tk-random_forest-demo\nmodel_name =\n\n\n# n.b. each time you run this cell with the same model_name, the model version will increase by one\nregistered_model = mlflow.register_model(f'runs:/{best_run_id}/model', model_name)\n\nWhich is great. And now you should be able to navigate to the MLFlow UI - and if you click on the ‘Models’ tab at the top of the page you should see your newly registered model waiting there on the list.",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Modelling"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_2_modelling.html#forward-pass-prediction",
    "href": "tutorials/hylode/vignette_2_modelling.html#forward-pass-prediction",
    "title": "Modelling",
    "section": "Forward pass prediction",
    "text": "Forward pass prediction\nWith the model and the lens loaded, the live_dataset from HyCastle makes it extremely straightforward to run the forward pass. (n.b. reusing the identical components to in our retrospective training)\n\nlive_df = live_dataset('T03')\nlive_df.shape\n\n\n# and inspecting the dataframe, note the most recent admission_dt\nlive_df.loc[:, ['episode_slice_id', 'admission_dt', 'bed_code', 'avg_heart_rate_1_24h']].sort_values('admission_dt', ascending=False).head()\n\nNow let’s try to run some patient-level predictions based on our saved model:\n\n# first we transform the live_df with our loaded_lens\nX_df = loaded_lens.transform(live_df)\nX_df.columns\n\n\n# making the predictions\npredictions = model.predict_proba(X_df.values)\n\n# adding the predictions to our live_df dataframe\nlive_df['prediction'] = predictions[:, 1]\nlive_df.loc[:, ['episode_slice_id', 'prediction']].head()\n\nWe can even then get a sense of how this segues into the aggregate prediction problem, using the AggregateDemandModel class:\n\nAggregateDemandModel??\n\n\nagg_demand = AggregateDemandModel()\nagg_predictions = agg_demand.predict(context=\"\", \n                                     model_input=live_df.loc[:, ['prediction']].rename(mapper={'prediction':'prediction_as_real'},axis=1))\nagg_predictions.plot()",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Modelling"
    ]
  },
  {
    "objectID": "tutorials/hylode/vignette_2_modelling.html#further-evaluation",
    "href": "tutorials/hylode/vignette_2_modelling.html#further-evaluation",
    "title": "Modelling",
    "section": "Further evaluation",
    "text": "Further evaluation\nAnother use case would be that having done our initial training, we still have plenty of work to do evaluating it’s performance. We give a very simple outline here of what that might look like.\nWe start by putting our two loaded components from MLFlow: loaded_tag_dict and loaded_lens together to rebuild our validation set.\n\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp_dir = Path(tmp)\n    \n    client.download_artifacts(model_info.run_id, './', tmp_dir)\n    \n    tag_dict_path = tmp_dir / 'tag_dict.yaml'\n    with open(tag_dict_path, 'r') as stream:\n        loaded_tag_dict = yaml.load(stream, Loader=yaml.FullLoader)\n        \nloaded_tag_dict\n\n\nloaded_valid_df = df[(loaded_tag_dict['start_valid_dt'] &lt; df['horizon_dt']) &\n                 (df['horizon_dt'] &lt; loaded_tag_dict['end_valid_dt'])]\n\nRecreating our dataset as follows:\n\nX_valid = loaded_lens.transform(loaded_valid_df)\ny_valid = loaded_valid_df['discharged_in_48hr'].astype(int)\n\n\n# then we have already loaded in our model in the previous section\nmodel\n\n\nwith mlflow.start_run(run_id=best_run_id):\n    \n    mlflow_log_tag_dict(tag_dict, 'tag_dict.yaml')\n    \n    # create a 2-column dataframe of the predicted probabilities and true label,\n    # for each patient in the validation set\n    eval_df = pd.DataFrame({\n                'predict_proba':model.predict_proba(X_valid.values)[:,1], \n                'label':y_valid.to_numpy().ravel()\n               }, \n        columns=['predict_proba','label'],\n        index=X_valid.index)   \n    eval_df['horizon_dt'] = loaded_valid_df.set_index('episode_slice_id')['horizon_dt']\n    \n    # write eval_df to csv and log in MLFlow\n    eval_path = tmp_path / 'eval.csv'\n    eval_df.to_csv(eval_path)\n    mlflow.log_artifact(eval_path)\n    \n    \n    # use eval_df to store a new metric\n    eval_log_loss = log_loss(eval_df['label'],eval_df['predict_proba'])\n    mlflow.log_metric('log_loss', eval_log_loss)\n    \n    \n    # save a new figure alongside our registered model\n    eval_confusion = confusion_matrix(m.predict(X_valid.values), y_valid.to_numpy())\n    disp = ConfusionMatrixDisplay(confusion_matrix=eval_confusion,\n                              display_labels=['discharged','remained_after_48hrs'])\n    \n    confusion_path = tmp_path / 'confusion_fig_2.png'\n    disp.plot(cmap=plt.cm.Blues).figure_.savefig(confusion_path)\n    mlflow.log_artifact(confusion_path)",
    "crumbs": [
      "Tutorials",
      "HYLODE",
      "Modelling"
    ]
  },
  {
    "objectID": "tutorials/five-safes/five-safes.html",
    "href": "tutorials/five-safes/five-safes.html",
    "title": "Safe data",
    "section": "",
    "text": "We follow the ‘Five Safes’ approach to managing data and information security. This means that we don’t rely on just the ‘safety’ of the data but also take into account the following:\n\n\n\nall individuals have substantive contracts or educational relationships with higher education or NHS institutions\nthose working need to have evidence of experience of working with such data (e.g. previous training, previous work with ONS, data safe havens etc.) or they need a supervisor who can has similar experience\nthose working need to undergo training in information governance and issues with statistical disclosure control (SDC)\n\n\n\n\n\nprojects must ‘serve the public good’\nprojects must meet relevant HRA and UCLH research and ethics approvals\nservice delivery work mandated as per usual trust processes\n\n\n\n\n\nworking at UCLH in the NHS on approved infrastructure\nUCLH local and remote desktops\nUCLH Data Science Desktop\nGeneric Application Development Environment (GADE)\n\n\n\n\n\noutputs (e.g. reports, figures and tables) must be non-disclosing\noutputs should remain on NHS systems initially\na copy of all outputs that are released externally (documents) should be stored in one central location so that there is visibility for all\n\n\n\n\n\ndirect identifiers (hospital numbers, NHS numbers, names etc) should be masked unless there is an explicit justification for their use\ndata releases are proportionate (e.g. limited by calendar periods, by patient cohort etc.)\nfurther work to obscure or mask the data is not necessary given the other safe guards (as per the recommendation by the UK data service)\n\n\n\n\n\nThe majority of this content is derived and adapted from the Safe Data Access Professional’s Working group, and we strongly recommend reviewing their handbook]\n\n\n\n\n\nPractically although this means that we are judging your data safety on more than just the qualities of the data, we are able to work with data that would otherwise be considered unsafe. The plot below demonstrates this by comparing the effort we would have to expend on safety if we wanted to release data on the internet. This means that we lose all the other 4 safes.\n\n\n\nSafety through disclosure control alone is much harder than using the other 4 ‘safes’",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Safe data"
    ]
  },
  {
    "objectID": "tutorials/five-safes/five-safes.html#five-safes",
    "href": "tutorials/five-safes/five-safes.html#five-safes",
    "title": "Safe data",
    "section": "",
    "text": "We follow the ‘Five Safes’ approach to managing data and information security. This means that we don’t rely on just the ‘safety’ of the data but also take into account the following:\n\n\n\nall individuals have substantive contracts or educational relationships with higher education or NHS institutions\nthose working need to have evidence of experience of working with such data (e.g. previous training, previous work with ONS, data safe havens etc.) or they need a supervisor who can has similar experience\nthose working need to undergo training in information governance and issues with statistical disclosure control (SDC)\n\n\n\n\n\nprojects must ‘serve the public good’\nprojects must meet relevant HRA and UCLH research and ethics approvals\nservice delivery work mandated as per usual trust processes\n\n\n\n\n\nworking at UCLH in the NHS on approved infrastructure\nUCLH local and remote desktops\nUCLH Data Science Desktop\nGeneric Application Development Environment (GADE)\n\n\n\n\n\noutputs (e.g. reports, figures and tables) must be non-disclosing\noutputs should remain on NHS systems initially\na copy of all outputs that are released externally (documents) should be stored in one central location so that there is visibility for all\n\n\n\n\n\ndirect identifiers (hospital numbers, NHS numbers, names etc) should be masked unless there is an explicit justification for their use\ndata releases are proportionate (e.g. limited by calendar periods, by patient cohort etc.)\nfurther work to obscure or mask the data is not necessary given the other safe guards (as per the recommendation by the UK data service)\n\n\n\n\n\nThe majority of this content is derived and adapted from the Safe Data Access Professional’s Working group, and we strongly recommend reviewing their handbook]",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Safe data"
    ]
  },
  {
    "objectID": "tutorials/five-safes/five-safes.html#five-safes-at-uclh",
    "href": "tutorials/five-safes/five-safes.html#five-safes-at-uclh",
    "title": "Safe data",
    "section": "",
    "text": "Practically although this means that we are judging your data safety on more than just the qualities of the data, we are able to work with data that would otherwise be considered unsafe. The plot below demonstrates this by comparing the effort we would have to expend on safety if we wanted to release data on the internet. This means that we lose all the other 4 safes.\n\n\n\nSafety through disclosure control alone is much harder than using the other 4 ‘safes’",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Safe data"
    ]
  },
  {
    "objectID": "tutorials/five-safes/five-safes.html#anonymisation-is-really-hard",
    "href": "tutorials/five-safes/five-safes.html#anonymisation-is-really-hard",
    "title": "Safe data",
    "section": "Anonymisation (is really hard)",
    "text": "Anonymisation (is really hard)\nMethods include Generalised Adversarial Networks, differentially-private Bayesian generative models, and Statistical Disclosure Control\n\nStatistical Disclosure Control\nSet thresholds for\n\nk-anonymity: counts the number of individuals identified by the intersection of key variables\nl-diversity: counts how varied other sensitive fields are within a k-anonymous group\n\nThen define\n\ndirect identifiers\nkey variables (indirect identifiers)\nsensitive fields\nnon-identifying variables\n\n\n\n\n\n\n\nFigure 1: Initial procedural steps to reduce re-identification risk and specifying a priori the statistical disclosure control thresholds.\n\n\n\n\n\n\n\n\n\nFigure 2: Algorithim that iteratively examines the disclosure risk and applies noise or aggregates data until this meets the thresholds specified above\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTypes of disclosure\n\nPrimary disclosure\n\nInferring the identity, and/or information about, a data subject from a single source of data.\n\nSecondary disclosure (‘attribute’)\n\nDeriving the identity, and/or information of, a data subjecting by combining two or more sources of information together.",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Safe data"
    ]
  },
  {
    "objectID": "tutorials/five-safes/five-safes.html#frequency-tables",
    "href": "tutorials/five-safes/five-safes.html#frequency-tables",
    "title": "Safe data",
    "section": "Frequency tables",
    "text": "Frequency tables\nRules-based - Minimum cell count - All counts should be unweighted\nPrinciples-based - Threshold is a ‘rule-of-thumb’ - The units and data being presented should be considered\nFrequencies can be presented in many different ways including tables, histograms, pie charts, bar charts. The guidance for frequency tables will also apply for these.",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Safe data"
    ]
  },
  {
    "objectID": "tutorials/five-safes/five-safes.html#graphs-and-figures",
    "href": "tutorials/five-safes/five-safes.html#graphs-and-figures",
    "title": "Safe data",
    "section": "Graphs and Figures",
    "text": "Graphs and Figures\nExample issues include\n\nhistograms: often low counts in the tails of the distribution, the maximum and minimum values may also be shown.\nscatter plots: by definition are plots of individuals (also residual plots); consider grouping",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Safe data"
    ]
  },
  {
    "objectID": "tutorials/five-safes/five-safes.html#four-eyes-principle",
    "href": "tutorials/five-safes/five-safes.html#four-eyes-principle",
    "title": "Safe data",
    "section": "Four eyes principle",
    "text": "Four eyes principle",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Safe data"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding.html",
    "href": "tutorials/onboarding/onboarding.html",
    "title": "Getting started",
    "section": "",
    "text": "Under construction!\n\n\n\nWe are currently building this page!\nIf you’re here, you’re likely about to start work with the UCL Health Algorithms Laboratory. This guide aims to explain or signpost everything you need to know to start analysing health data at UCLH. (general aims, translational data science, etc)",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding.html#how-are-health-data-stored-at-uclh",
    "href": "tutorials/onboarding/onboarding.html#how-are-health-data-stored-at-uclh",
    "title": "Getting started",
    "section": "How are health data stored at UCLH?",
    "text": "How are health data stored at UCLH?\n(the importance of live and recent data, etc)\n\nWorking with EPIC\n(link to page on further details on EPIC)\n\n\nWorking with EMAP\n(link to EMAP page)",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding.html#how-are-health-data-analysed",
    "href": "tutorials/onboarding/onboarding.html#how-are-health-data-analysed",
    "title": "Getting started",
    "section": "How are health data analysed?",
    "text": "How are health data analysed?\n\nWorking safely with health data\nfive safes etc\n\n\nResources to learn {stats / ML / SQL / python}\n(link to Resources tab?) - Link to Data Science for Docs material?\n\n\nHYLODE\n(link to the full explanation page) (HySys diagrams, FlowEHR, etc)",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding.html#what-do-i-need-to-get-started",
    "href": "tutorials/onboarding/onboarding.html#what-do-i-need-to-get-started",
    "title": "Getting started",
    "section": "What do I need to get started?",
    "text": "What do I need to get started?\n\nSetting up your computer\n\npython\ngit\nR\n(onboarding page for adding a User page to this website etc too)\n\n\n\nUCLH access required\n\nnames of people to get involved with",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding.html#what-is-the-data-clinic",
    "href": "tutorials/onboarding/onboarding.html#what-is-the-data-clinic",
    "title": "Getting started",
    "section": "What is the Data Clinic?",
    "text": "What is the Data Clinic?\n\nlinks",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Getting started"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding-1.html",
    "href": "tutorials/onboarding/onboarding-1.html",
    "title": "Onboarding 1",
    "section": "",
    "text": "I’m not sure how you got here, but whatever happened then ‘Welcome’.\nWe assume that you’re here because you are going to start work with our data science team. This lesson will set-out some pre-requisites, and series of short lessons. Your first task will be to learn how to use GitHub for collaboration.",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Onboarding 1"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding-1.html#congratulations",
    "href": "tutorials/onboarding/onboarding-1.html#congratulations",
    "title": "Onboarding 1",
    "section": "",
    "text": "I’m not sure how you got here, but whatever happened then ‘Welcome’.\nWe assume that you’re here because you are going to start work with our data science team. This lesson will set-out some pre-requisites, and series of short lessons. Your first task will be to learn how to use GitHub for collaboration.",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Onboarding 1"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding-1.html#aim",
    "href": "tutorials/onboarding/onboarding-1.html#aim",
    "title": "Onboarding 1",
    "section": "Aim",
    "text": "Aim\nYou are going to create your user own user page, and publish that to the lab website.\n\nPre-requisites\n\nyou should be working from your own computer\nyou will need access to GitHub including a user account\nyou will need the following applications.\n\na text editor. We recommend Visual Studio Code because it’s free, cross-platform, well supported, and generally works well. But feel free to use any editor you like.\na ‘git’ graphical user application (GUI). You could try GitHub desktop. You could also just use the integrated git tools within VS Code.\na terminal application (e.g. ‘Terminal.app’ on Mac OS).\n\nyou should know how to write in ‘Markdown’. See the background here, or review their 60 second(!) lesson, or run through the 10 minute interactive tutorial.\n\nIf this is already all new to you then\n\ncheck out Software Carpentry’s tutorial Unix Shell (lessons 1-3 will get you started)\nsimilarly review their tutorial on Version Control with Git. You will need to run through lessons 1-3 but the ‘juice’ starts from Lesson 4.\n\n\n\nLesson plan\n\nSet-up this lesson &lt;-- you are here\nInstall Quarto (our documentation and website system) the next lesson\nWrite and publish your first page the last lesson",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Onboarding 1"
    ]
  },
  {
    "objectID": "tutorials/onboarding/onboarding-1.html#lesson-1",
    "href": "tutorials/onboarding/onboarding-1.html#lesson-1",
    "title": "Onboarding 1",
    "section": "Lesson 1",
    "text": "Lesson 1\nGiven the pre-requisites above then you need to clone this repository to your local machine. We’re going to assume you’re using GitHub desktop where you will need to select the File &gt; Clone Repository… menu, and then the URL tab of the dialog box. Paste the URL of the repository (https://github.com/uclhal/uclhal) into the first text field, and then choose a local path (on your machine) to store your work.\n\nThe step above will clone the main branch of the repository. This branch represents the version of the work in the repository that we use to run the website. You should never edit or change the main branch directly. Instead we ‘check out’ a separate branch, prepare our work safely there, and then create a ‘pull request’ to ask a collaborator to merge work from our branch into the main branch. This review process protects us all.\nFor now, you just need to checkout a new branches after you clone the repository. The git command would be git checkout -b my-new-branch, but it’s typically easier to use the git GUI (i.e. GitHub desktop or similar) that you installed above.",
    "crumbs": [
      "Tutorials",
      "Onboarding",
      "Onboarding 1"
    ]
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "UCL Health Algorithms Laboratory",
    "section": "",
    "text": "Order By\n       Default\n         \n          Publication\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n     2022 \n      Circulation  \n    Knight R, Walker V, Ip S, Cooper J, ..., Sterne J\n    Association of COVID-19 With Major Arterial and Venous Thrombotic Diseases: A Population-Wide Cohort Study of 48 Million Adults in England and Wales\n  \n\n  \n     2022 \n      The Lancet Digital Health  \n    Thygesen J, Tomlinson C, Hollings S, Mizani M, ..., Zuccolo L\n    COVID-19 trajectories among 57 million adults in England: a cohort study using electronic health records\n  \n\n  \n     2022 \n      Frontiers in Digital Health  \n    Harris S, Bonnici T, Keen T, Lilaonitkul W, ..., Swanepoel N\n    Clinical deployment environments: Five pillars of translational machine learning for health\n  \n\n  \n     2022 \n      Scientific Reports  \n    Wilson M, Rashan A, Klapaukh R, Asselbergs F, Harris S\n    Clinician preference instrumental variable analysis of the effectiveness of magnesium supplementation for atrial fibrillation prophylaxis in critical care\n  \n\n  \n     2022 \n        \n    Data Study Group Team \n    Data Study Group Final Report: University of Birmingham\n  \n\n  \n     2022 \n        \n    Pineda-Moncusí M, Allery F, Delmestri A, Bolton T, ..., Khalid S\n    Digital ethnicity data in population-wide electronic health records in England: a description of completeness, coverage, and granularity of diversity\n  \n\n  \n     2022 \n      BMJ Open  \n    Wilson M, Asselbergs F, Miguel R, Brealey D, Harris S\n    Embedded point of care randomisation for evaluating comparative effectiveness questions: PROSPECTOR-critical care feasibility study protocol\n  \n\n  \n     2022 \n      2022 Symposium on Eye Tracking Research and Applications  \n    Al-Hindawi A, Vizcaychipi M, Demiris Y\n    Faster, Better Blink Detection through Curriculum Learning by Augmentation\n  \n\n  \n     2022 \n      British Journal of Anaesthesia  \n    Wilson M, Asselbergs F, Harris S\n    Learning from individualised variation for evidence generation within a learning health system\n  \n\n  \n     2022 \n      npj Digital Medicine  \n    King Z, Farrington J, Utley M, Kung E, ..., Crowe S\n    Machine learning for real-time aggregated prediction of hospital admission for emergency patients\n  \n\n  \n     2022 \n      European Heart Journal  \n    Chen Y, Harris S, Rogers Y, Ahmad T, Asselbergs F\n    Nudging within learning health systems: next generation decision support to improve cardiovascular care\n  \n\n  \n     2022 \n      BMJ Open  \n    Kohler K, Nwe Myint P, Wynn S, Komashie A, ..., Bashford T\n    Systems approach to improving traumatic brain injury care in Myanmar: a mixed-methods study from lived experience to discrete event simulation\n  \n\n  \n     2022 \n      The Lancet Haematology  \n    Ghorashian S, Jacoby E, De Moerloose B, Rives S, ..., Baruchel A\n    Tisagenlecleucel therapy for relapsed or refractory B-cell acute lymphoblastic leukaemia in infants and children younger than 3 years of age at screening: an international, multicentre, retrospective cohort study\n  \n\n  \n     2022 \n      ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)  \n    Al-Hindawi A, Vizcaychipi M, Demiris Y\n    What Is The Patient Looking At? Robust Gaze-Scene Intersection Under Free-Viewing Conditions\n  \n\n  \n     2021 \n        \n    Handy A, Wood A, Sudlow C, Tomlinson C, ..., Denaxas S\n    A nationwide deep learning pipeline to predict stroke and COVID-19 death in atrial fibrillation\n  \n\n  \n     2021 \n      BMJ Case Reports CP  \n    Flower L, Bares Z, Santiapillai G, Harris S\n    Acute ST-segment elevation myocardial infarction secondary to vaccine-induced immune thrombosis with thrombocytopaenia (VITT)\n  \n\n  \n     2021 \n      BMC Medical Informatics and Decision Making  \n    Lai A, Chang W, Parisinos C, Katsoulis M, ..., Hemingway H\n    An informatics consult approach for generating clinical evidence for treatment decisions\n  \n\n  \n     2021 \n      2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)  \n    Al-Hindawi A, Vizcaychipi M, Demiris Y\n    Continuous Non-Invasive Eye Tracking In Intensive Care\n  \n\n  \n     2021 \n      Critical Care Medicine  \n    Shah A, MacCallum N, Harris S, Brealey D, ..., Singer M\n    Descriptors of Sepsis Using the Sepsis-3 Criteria: A Cohort Study in Critical Care Units Within the U.K. National Institute for Health Research Critical Care Health Informatics Collaborative*\n  \n\n  \n     2021 \n      American Journal of Respiratory and Critical Care Medicine  \n    Harris S\n    I Don't Want My Algorithm to Die in a Paper: Detecting Deteriorating Patients Early\n  \n\n  \n     2021 \n      Anaesthesia  \n    Hashim S, Wong D, Farmer L, Harris S, Moonesinghe S\n    Perceptions of UK clinicians towards postoperative critical care\n  \n\n  \n     2021 \n      Future Healthcare Journal  \n    Newcombe V, Coats T, Dark P, Gordon A, ..., Singer M\n    The future of acute and emergency care\n  \n\n  \n     2021 \n      British Journal of Anaesthesia  \n    Harris S, Palmer E, Fong K\n    Trials in pandemics: here we go again?\n  \n\n  \n     2021 \n      Journal of the Royal Society of Medicine  \n    Gurdasani D, Bhatt S, Costello A, Denaxas S, ..., Pagel C\n    Vaccinating adolescents against SARS-CoV-2 in England: a risk–benefit analysis\n  \n\n  \n     2020 \n      BMJ Open  \n    Kohler K, Ercole A\n    Can network science reveal structure in a complex healthcare system? A network analysis using data from emergency surgical services\n  \n\n  \n     2020 \n      British Journal of Anaesthesia  \n    Krishnamoorthy V, Wong D, Wilson M, Raghunathan K, ..., Harris S\n    Causal inference in perioperative medicine observational research: part 1, a graphical introduction\n  \n\n  \n     2020 \n      British Journal of Anaesthesia  \n    Krishnamoorthy V, McLean D, Ohnuma T, Harris S, ..., Raghunathan K\n    Causal inference in perioperative medicine observational research: part 2, advanced methods\n  \n\n  \n     2020 \n      PLOS Medicine  \n    Wong D, Harris S, Sahni A, Bedford J, ..., Moonesinghe S\n    Developing and validating subjective and objective risk-assessment measures for predicting mortality after major surgery: An international prospective cohort study\n  \n\n  \n     2020 \n      Critical Care  \n    Beane A, Dondorp A, Taqi A, Ahsan A, ..., Lubell Y\n    Establishing a critical care network in Asia to improve care for critically ill patients in low- and middle-income countries\n  \n\n  \n     2020 \n      The Lancet  \n    Banerjee A, Pasea L, Harris S, Gonzalez-Izquierdo A, ..., Hemingway H\n    Estimating excess 1-year mortality associated with the COVID-19 pandemic according to underlying conditions and age: a population-based cohort study\n  \n\n  \n     2020 \n      IEEE Journal of Biomedical and Health Informatics  \n    Tissot H, Shah A, Brealey D, Harris S, ..., Asselbergs F\n    Natural Language Processing for Mimicking Clinical Trial Recruitment in Critical Care: A Semi-Automated Simulation Based on the LeoPARDS Trial\n  \n\n  \n     2020 \n      British Journal of Anaesthesia  \n    Post B, Palmer E, Harris S, Singer M, Martin D\n    Oxygenation of the critically ill in selected intensive care units in the UK: are we usual?\n  \n\n  \n     2020 \n      Journal of Critical Care  \n    Domizi R, Calcinaro S, Harris S, Beilstein C, ..., Singer M\n    Relationship between norepinephrine dose, tachycardia and outcome in septic shock: A multicentre evaluation\n  \n\n  \n     2020 \n      Journal of the Royal Statistical Society: Series A (Statistics in Society)  \n    Keele L, Harris S, Pimentel S, Grieve R\n    Stronger instruments and refined covariate balance in an observational study of the effectiveness of prompt admission to intensive care units\n  \n\n  \n     2020 \n      Intensive Care Medicine  \n    Arulkumaran N, Wright T, Harris S, Singer M\n    Uncontrolled interventions during pandemics: a missed learning opportunity?\n  \n\n  \n     2019 \n      JAMA Network Open  \n    Grieve R, O’Neill S, Basu A, Keele L, ..., Harris S\n    Analysis of Benefit of Intensive Care Unit Transfer for Deteriorating Ward Patients: A Patient-Centered Approach to Clinical Evaluation\n  \n\n  \n     2019 \n      Medical Care  \n    Keele L, Harris S, Grieve R\n    Does Transfer to Intensive Care Units Reduce Mortality? A Comparison of an Instrumental Variables Design to Risk Adjustment\n  \n\n  \n     2019 \n      Critical Care  \n    Palmer E, Klapaukh R, Harris S, Singer M\n    Intelligently learning from data\n  \n\n  \n     2019 \n      British Journal of Anaesthesia  \n    Wong D, Popham S, Wilson A, Barneto L, ..., Dale-Gandar J\n    Postoperative critical care and high-acuity care provision in the United Kingdom, Australia, and New Zealand\n  \n\n  \n     2019 \n      Journal of the American Statistical Association  \n    Kennedy E, Harris S, Keele L\n    Survivor-Complier Effects in the Presence of Selection on Treatment, With Application to a Study of Prompt ICU Admission\n  \n\n  \n     2019 \n      British Journal of Anaesthesia  \n    Barnes J, Hunter J, Harris S, Shankar-Hari M, ..., Johnson M\n    Systematic review and consensus definitions for the Standardised Endpoints in Perioperative Medicine (StEP) initiative: infection and sepsis\n  \n\n  \n     2019 \n      American Journal of Respiratory and Critical Care Medicine  \n    Palmer E, Post B, Klapaukh R, Marra G, ..., Harris S\n    The Association between Supraphysiologic Arterial Oxygen Levels and Mortality in Critically Ill Patients. A Multicenter Observational Cohort Study\n  \n\n  \n     2018 \n      British Journal of Anaesthesia  \n    Wong D, Harris S, Moonesinghe S\n    Cancelled operations: a 7-day cohort study of planned adult inpatient surgery in 245 UK National Health Service hospitals\n  \n\n  \n     2018 \n      International Journal of Medical Informatics  \n    Harris S, Shi S, Brealey D, MacCallum N, ..., Singer M\n    Critical Care Health Informatics Collaborative (CCHIC): Data, tools and methods for reproducible research: A multi-centre UK intensive care database\n  \n\n  \n     2018 \n      British Journal of Hospital Medicine  \n    Pickard D, Harris D\n    High flow nasal oxygen therapy\n  \n\n  \n     2018 \n      Intensive Care Medicine  \n    Harris S, Singer M, Sanderson C, Grieve R, ..., Rowan K\n    Impact on mortality of prompt admission to critical care for deteriorating ward patients: an instrumental variable analysis using critical care bed strain\n  \n\n  \n     2018 \n      British Journal of Anaesthesia  \n    Wong D, Sahni A, Bedford J, Harris S, Moonesinghe S\n    Man vs machine: how good are clinicians at predicting perioperative risk?\n  \n\n  \n     2018 \n      Anaesthesia  \n    Palmer E, Ciechanowicz S, Reeve A, Harris S, ..., Sultan P\n    Operating room-to-incision interval and neonatal outcome in emergency caesarean section: a retrospective 5-year cohort study\n  \n\n  \n     2018 \n      PLOS ONE  \n    Meiring C, Dixit A, Harris S, MacCallum N, ..., Ercole A\n    Optimal intensive care outcome prediction over time using machine learning\n  \n\n  \n     2017 \n      The Journal of Open Source Software  \n    Shi S, Pérez-Suárez D, Harris S, MacCallum N, ..., Hetherington J\n    Critical care data processing tools\n  \n\n  \n     2017 \n      BMJ Open  \n    Moonesinghe S, Wong D, Farmer L, Shawyer R, ..., Harris S\n    SNAP-2 EPICCS: the second Sprint National Anaesthesia Project—EPIdemiology of Critical Care after Surgery: protocol for an international observational cohort study\n  \n\n  \n     2017 \n      American Journal of Respiratory and Critical Care Medicine  \n    Harris S\n    Sepsis-3: Syndromes and Empiricism in the Age of Big Data\n  \n\n  \n     2015 \n      Kidney International  \n    Harris S, Lewington A, Harrison D, Rowan K\n    Relationship between patients’ outcomes and the changes in serum creatinine and urine output and RIFLE classification in a large critical care cohort database\n  \n\n  \n     2014 \n      British Journal of Anaesthesia  \n    Moonesinghe S, Harris S, Mythen M, Rowan K, ..., Grocott M\n    Survival after postoperative morbidity: a longitudinal observational cohort study †\n  \n\n  \n     2010 \n      British Journal of Anaesthesia  \n    Ackland G, Harris S, Ziabari Y, Grocott M, Mythen M\n    Revised cardiac risk index and postoperative morbidity after elective orthopaedic surgery: a prospective cohort study\n  \n\n  \n     2006 \n      Bulletin of the World Health Organization  \n    Balasegaram M, Harris S, Checchi F, Ghorashian S, ..., Karunakara U\n    Melarsoprol versus eflornithine for treating late-stage Gambian trypanosomiasis in the Republic of the Congo\n  \n\n  \n     2006 \n      Bulletin of the World Health Organization  \n    Balasegaram M, Harris S, Checchi F, Hamel C, Karunakara U\n    Treatment outcomes and risk factors for relapse in patients with early-stage human African trypanosomiasis (HAT) in the Republic of the Congo\n  \n\n  \n     2003 \n      Postgraduate Medical Journal  \n    Harris S\n    An unusual electrocardiographic abnormality\n  \n\n  \n     2003 \n      The Lancet  \n    Syed H, Harris S, Dubrey S\n    Grandma's eyes\n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "people/associate/hanlon_daniel.html",
    "href": "people/associate/hanlon_daniel.html",
    "title": "Daniel Hanlon",
    "section": "",
    "text": "I am a graduate entry medical student in RCSI Dublin, Ireland. Before studying medicine I worked as a software engineer for Microsoft, completed an MSc Artificial Intelligence for Medicine & Medical Research and BSc Computer Science from University College Dublin. I am interested in translational research using electronic health records and critical care. Currently, I am working with other members of the lab to model ‘ward strain’ and how it relates to patient deterioration."
  },
  {
    "objectID": "people/associate/hanlon_daniel.html#biography",
    "href": "people/associate/hanlon_daniel.html#biography",
    "title": "Daniel Hanlon",
    "section": "",
    "text": "I am a graduate entry medical student in RCSI Dublin, Ireland. Before studying medicine I worked as a software engineer for Microsoft, completed an MSc Artificial Intelligence for Medicine & Medical Research and BSc Computer Science from University College Dublin. I am interested in translational research using electronic health records and critical care. Currently, I am working with other members of the lab to model ‘ward strain’ and how it relates to patient deterioration."
  },
  {
    "objectID": "people/clinical/hrishee_vaidya.html",
    "href": "people/clinical/hrishee_vaidya.html",
    "title": "Hrisheekesh Vaidya",
    "section": "",
    "text": "I am a data science fellow in critical care and an out-of-programme IMT3. I am currently working on pre-operative prediction of ICU admission after elective surgery, using machine learning methods applied to electronic healthcare records. I am also working with Ruaraidh, Sarah, and Tim to develop the Data Clinic, a novel service to provide timely data to clinicans for the use in audit and quality improvement.\nLong-term, I hope to persue a career in academic and clinical medicine with a focus on applying machine learning techniques to challenges in critical care."
  },
  {
    "objectID": "people/clinical/hrishee_vaidya.html#biography",
    "href": "people/clinical/hrishee_vaidya.html#biography",
    "title": "Hrisheekesh Vaidya",
    "section": "",
    "text": "I am a data science fellow in critical care and an out-of-programme IMT3. I am currently working on pre-operative prediction of ICU admission after elective surgery, using machine learning methods applied to electronic healthcare records. I am also working with Ruaraidh, Sarah, and Tim to develop the Data Clinic, a novel service to provide timely data to clinicans for the use in audit and quality improvement.\nLong-term, I hope to persue a career in academic and clinical medicine with a focus on applying machine learning techniques to challenges in critical care."
  },
  {
    "objectID": "people/clinical/hla_teddy.html",
    "href": "people/clinical/hla_teddy.html",
    "title": "Teddy Tun Win HLA",
    "section": "",
    "text": "Teddy Tun Win Hla is a specialist trainee in intensive care medicine at London Deanery with an interest in respiratory failure and mechanical ventilation. He is an honorary research assitant at UCL Institue for Health Informatics\nThrough the use of quantitative analysis,mathematical modelling, and data visualisation, he aims to enhance patient care at bed-side, particularly in dynamic and complex physiological systems such as in critically unwell patients.\nHis current work includes real-time data visualisation and analysis of lung mechanics for critically unwell patients."
  },
  {
    "objectID": "people/clinical/hla_teddy.html#biography",
    "href": "people/clinical/hla_teddy.html#biography",
    "title": "Teddy Tun Win HLA",
    "section": "",
    "text": "Teddy Tun Win Hla is a specialist trainee in intensive care medicine at London Deanery with an interest in respiratory failure and mechanical ventilation. He is an honorary research assitant at UCL Institue for Health Informatics\nThrough the use of quantitative analysis,mathematical modelling, and data visualisation, he aims to enhance patient care at bed-side, particularly in dynamic and complex physiological systems such as in critically unwell patients.\nHis current work includes real-time data visualisation and analysis of lung mechanics for critically unwell patients."
  },
  {
    "objectID": "people/clinical/hla_teddy.html#selected-works",
    "href": "people/clinical/hla_teddy.html#selected-works",
    "title": "Teddy Tun Win HLA",
    "section": "Selected works",
    "text": "Selected works\nOral Presentation : European Society of Intensive Care Medicine 2021 Datathon Grand finale\nCritical care transfers and COVID-19: Managing capacity challenges through critical care network"
  },
  {
    "objectID": "people/academic/tomlinson_chris.html",
    "href": "people/academic/tomlinson_chris.html",
    "title": "Chris Tomlinson",
    "section": "",
    "text": "Chris Tomlinson is an Anaesthetics & Intensive Care registrar undertaking a PhD at the UKRI UCL Centre for Doctoral Training in AI-enabled Healthcare Systems.\nCombining technical expertise in epidemiology, data science and machine/deep learning with an extensive background in clinical medicine and physiological research he uses real world evidence to uncover new insights of critical relevance to patients, clinicians and policymakers. Most recently this has involved creating novel COVID-19 phenotypes from linked-EHR data of 57 million individuals in England (now published in Lancet Digital Health).\nHe is interested in the application of AI methodologies to disentangle the complex interactions between organ systems, diseases, individuals and healthcare services to advance our scientific understanding and translate knowledge into tangible benefits to patient care."
  },
  {
    "objectID": "people/academic/tomlinson_chris.html#biography",
    "href": "people/academic/tomlinson_chris.html#biography",
    "title": "Chris Tomlinson",
    "section": "",
    "text": "Chris Tomlinson is an Anaesthetics & Intensive Care registrar undertaking a PhD at the UKRI UCL Centre for Doctoral Training in AI-enabled Healthcare Systems.\nCombining technical expertise in epidemiology, data science and machine/deep learning with an extensive background in clinical medicine and physiological research he uses real world evidence to uncover new insights of critical relevance to patients, clinicians and policymakers. Most recently this has involved creating novel COVID-19 phenotypes from linked-EHR data of 57 million individuals in England (now published in Lancet Digital Health).\nHe is interested in the application of AI methodologies to disentangle the complex interactions between organ systems, diseases, individuals and healthcare services to advance our scientific understanding and translate knowledge into tangible benefits to patient care."
  },
  {
    "objectID": "people/academic/alhindawi_ahmed.html",
    "href": "people/academic/alhindawi_ahmed.html",
    "title": "Ahmed Al-Hindawi",
    "section": "",
    "text": "Ahmed Al-Hindawi is an NHS Critical Care Doctor and NIHR Academic Clinical Lecturer at the UCL Bloomsbury Institute of Intensive Care. He is interested in novel physiological measurement utilising the latest in AI, machine learning, and domain specialisation in critical care. His Ph.D. at Imperial College London focused on the development and validation of a novel state-of-the-art AI-based eye tracker. This facilitated the exploration of eye-movements for the diagnosis of delirium in critical care. His current project involves the non-invasive acquisition of Ocular Microtremor for the objective assessment of sedation in critical care where he’s the PI on OCTAVE, and holds a patent for the method."
  },
  {
    "objectID": "people/academic/alhindawi_ahmed.html#biography",
    "href": "people/academic/alhindawi_ahmed.html#biography",
    "title": "Ahmed Al-Hindawi",
    "section": "",
    "text": "Ahmed Al-Hindawi is an NHS Critical Care Doctor and NIHR Academic Clinical Lecturer at the UCL Bloomsbury Institute of Intensive Care. He is interested in novel physiological measurement utilising the latest in AI, machine learning, and domain specialisation in critical care. His Ph.D. at Imperial College London focused on the development and validation of a novel state-of-the-art AI-based eye tracker. This facilitated the exploration of eye-movements for the diagnosis of delirium in critical care. His current project involves the non-invasive acquisition of Ocular Microtremor for the objective assessment of sedation in critical care where he’s the PI on OCTAVE, and holds a patent for the method."
  },
  {
    "objectID": "people/academic/wilson_matt.html",
    "href": "people/academic/wilson_matt.html",
    "title": "Matt Wilson",
    "section": "",
    "text": "Matt graduated from St Bartholomew’s and The Royal London Medical School in 2011 and works as an Anaesthetic registrar in London. He completed a MSc in Health Data Science in 2018 and is currently a final year PhD student at UCL funded through an MRC Doctoral Training Partnership grant.\nHis PhD is focused on creating a research pipeline for integrating randomised trials into routine clinical care by utilising modern electronic health record systems. This currently involves conducting observational analyses using natural experimental methods and investigating the feasibility of conducting digitally-enabled clinical trials to generate evidence from routine clinical practice in anaesthesia and critical care."
  },
  {
    "objectID": "people/academic/wilson_matt.html#biography",
    "href": "people/academic/wilson_matt.html#biography",
    "title": "Matt Wilson",
    "section": "",
    "text": "Matt graduated from St Bartholomew’s and The Royal London Medical School in 2011 and works as an Anaesthetic registrar in London. He completed a MSc in Health Data Science in 2018 and is currently a final year PhD student at UCL funded through an MRC Doctoral Training Partnership grant.\nHis PhD is focused on creating a research pipeline for integrating randomised trials into routine clinical care by utilising modern electronic health record systems. This currently involves conducting observational analyses using natural experimental methods and investigating the feasibility of conducting digitally-enabled clinical trials to generate evidence from routine clinical practice in anaesthesia and critical care."
  },
  {
    "objectID": "people/academic/rashan_aasiyah.html",
    "href": "people/academic/rashan_aasiyah.html",
    "title": "Aasiyah Rashan",
    "section": "",
    "text": "I am a data scientist undertaking a PhD at the UKRI UCL Centre for Doctoral Training in AI-enabled Healthcare Systems. I am also a data analyst at CritCare Asia Africa\nMy PhD focuses on establishing a harmonised FAIR dataset with an OMOP common data model for CritCare Asia Africa and then assessing the usability of that dataset by combining CritCare Asia Africa and UK critical care data to look at the timing of tracheostomies in ICU patients. My specific methods focus is on (a) causal inference and (b) reproducibility of biological findings across health care systems."
  },
  {
    "objectID": "people/academic/rashan_aasiyah.html#biography",
    "href": "people/academic/rashan_aasiyah.html#biography",
    "title": "Aasiyah Rashan",
    "section": "",
    "text": "I am a data scientist undertaking a PhD at the UKRI UCL Centre for Doctoral Training in AI-enabled Healthcare Systems. I am also a data analyst at CritCare Asia Africa\nMy PhD focuses on establishing a harmonised FAIR dataset with an OMOP common data model for CritCare Asia Africa and then assessing the usability of that dataset by combining CritCare Asia Africa and UK critical care data to look at the timing of tracheostomies in ICU patients. My specific methods focus is on (a) causal inference and (b) reproducibility of biological findings across health care systems."
  },
  {
    "objectID": "people/academic/kohler_katharina.html",
    "href": "people/academic/kohler_katharina.html",
    "title": "Katharina Kohler",
    "section": "",
    "text": "I am an NIHR Research DSE Fellow working within the wider UCL Health Algorithms group. I am interested in peri-operative health systems both within the UK and a global health setting. My position as Locum Consultant in Anaesthesia at Cambridge University Hospitals gives me the clinical background to inform my research.\nCombining technical expertise in data science from my previous experience in physics research with my background in peri-operative medicine I am keen to discover the how aspects of health systems affect patient care quality and outcomes. I am particularly interested in analysing healthcare systems and resource usage both within the high-income data-rich environment of electronic health records and the low-income data-poor setting of low- and middle income (LMIC) acute care services. Within the data science setting I hope to employ quantitative methods from other areas of data science such as network analysis, machine learning and simulation to develop clinically founded models that allow us to understand strain/stress within healthcare systems and effectively target improvement and development in the peri-operative care.\nSome of my recent work published in a variety of journals:\nMulti-centre, international study on COVID-19 patients and the effects of treatment on the development of secondary infections\nSystems approach to improving traumatic brain injury care in Myanmar: a mixed-methods study from lived experience to discrete event simulation\nUse of network analysis to model the effects of the SARS Cov2 pandemic on acute patient care within a healthcare system\nNetwork science to investigate the structure of a complex healthcare system - a network analysis using data from emergency surgical services\nComputational model of cerebral oxygenation after traumatic brain injury and the implications for Rescuing Hypoxic Tissue"
  },
  {
    "objectID": "people/academic/kohler_katharina.html#biography",
    "href": "people/academic/kohler_katharina.html#biography",
    "title": "Katharina Kohler",
    "section": "",
    "text": "I am an NIHR Research DSE Fellow working within the wider UCL Health Algorithms group. I am interested in peri-operative health systems both within the UK and a global health setting. My position as Locum Consultant in Anaesthesia at Cambridge University Hospitals gives me the clinical background to inform my research.\nCombining technical expertise in data science from my previous experience in physics research with my background in peri-operative medicine I am keen to discover the how aspects of health systems affect patient care quality and outcomes. I am particularly interested in analysing healthcare systems and resource usage both within the high-income data-rich environment of electronic health records and the low-income data-poor setting of low- and middle income (LMIC) acute care services. Within the data science setting I hope to employ quantitative methods from other areas of data science such as network analysis, machine learning and simulation to develop clinically founded models that allow us to understand strain/stress within healthcare systems and effectively target improvement and development in the peri-operative care.\nSome of my recent work published in a variety of journals:\nMulti-centre, international study on COVID-19 patients and the effects of treatment on the development of secondary infections\nSystems approach to improving traumatic brain injury care in Myanmar: a mixed-methods study from lived experience to discrete event simulation\nUse of network analysis to model the effects of the SARS Cov2 pandemic on acute patient care within a healthcare system\nNetwork science to investigate the structure of a complex healthcare system - a network analysis using data from emergency surgical services\nComputational model of cerebral oxygenation after traumatic brain injury and the implications for Rescuing Hypoxic Tissue"
  }
]